#!/usr/bin/env python3

import os
import copy
import os
import torch
import transformers
import argparse

from lm_polygraph.utils.manager import UEManager
from lm_polygraph.utils.dataset import Dataset
from lm_polygraph.utils.model import WhiteboxModel
from lm_polygraph.utils.processor import Logger
from lm_polygraph.generation_metrics import RougeMetric, WERTokenwiseMetric, BartScoreSeqMetric, ModelScoreSeqMetric, ModelScoreTokenwiseMetric
from lm_polygraph.estimators import *
from lm_polygraph.ue_metrics import ReversedPairsProportion, PredictionRejectionArea, RiskCoverageCurveAUC


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str,
                        default='/Users/ekaterinafadeeva/work/data/uncertainty_datasets/triviaqa.csv')
    parser.add_argument("--train_dataset", type=str, default=None)
    parser.add_argument("--background_train_dataset", type=str, default="allenai/c4")
    parser.add_argument("--model", type=str, default='databricks/dolly-v2-3b')
    parser.add_argument("--use_density_based_ue", type=bool, default=True, action=argparse.BooleanOptionalAction)
    parser.add_argument("--ignore_exceptions", type=bool, default=True, action=argparse.BooleanOptionalAction)
    parser.add_argument("--subsample_background_train_dataset", type=int, default=500)
    parser.add_argument("--subsample_train_dataset", type=int, default=1500)
    parser.add_argument("--subsample_eval_dataset", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--seed", nargs='+', type=int)
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--save_path", type=str,
                        default='/home/petrakov/emnlp/lm-polygraph/workdir/save')
    parser.add_argument("--cache_path", type=str,
                        default='/home/petrakov/emnlp/lm-polygraph/workdir')
    parser.add_argument("--input_column", type=str, default='question')
    parser.add_argument("--output_column", type=str, default='answer')
    parser.add_argument("--prompt", type=str, default='')
    args = parser.parse_args()

    if args.seed is None or len(args.seed) == 0:
        args.seed = [1]

    device = args.device
    if device is None:
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

    for seed in args.seed:
        print('=' * 150)
        print('SEED:', seed)
        transformers.set_seed(seed)
        load_in_8bit = ('cuda' in device) and any(s in args.model for s in ['7b', '12b', '13b'])
        model = WhiteboxModel.from_pretrained(
            args.model,
            device=device,
            load_in_8bit=load_in_8bit,
        )
        print(device, args.model, load_in_8bit)

        dataset, max_new_tokens = Dataset.load(
            args.dataset,
            args.input_column,
            args.output_column,
            batch_size=args.batch_size,
            prompt=args.prompt,
            split="test",
        )

        if args.use_density_based_ue:
            if (args.train_dataset is not None) and (args.train_dataset != args.dataset):
                train_dataset, _ = Dataset.load(
                    args.train_dataset,
                    args.input_column,
                    args.output_column,
                    batch_size=args.batch_size,
                    prompt=args.prompt,
                    split="train",
                )
            elif not args.dataset.endswith("csv"):
                train_dataset, _ = Dataset.load(
                    args.dataset,
                    args.input_column,
                    args.output_column,
                    batch_size=args.batch_size,
                    prompt=args.prompt,
                    split="train",
                    size=10_000,
                )
            else:
                X_train, X_test, y_train, y_test = dataset.train_test_split(
                    test_size=0.7,
                    seed=seed,
                    split="eval"
                )
                train_dataset = Dataset(x=X_train, y=y_train, batch_size=args.batch_size)
            background_train_dataset, _ = Dataset.load(
                    args.background_train_dataset,
                    'text', 'url',
                    batch_size=args.batch_size,
                    size=100_000,
                    split="train",
                    data_files="en/c4-train.00000-of-01024.json.gz",
            )

            big_model = any(s in args.model for s in ['3b', '7b', '12b', '13b'])
            if args.subsample_train_dataset != -1:
                size = args.subsample_train_dataset if not big_model else args.subsample_train_dataset//3
                train_dataset.subsample(size, seed=seed)
            if args.subsample_background_train_dataset != -1:
                size = args.subsample_background_train_dataset if not big_model else args.subsample_background_train_dataset//3
                background_train_dataset.subsample(size, seed=seed)
                
            train_dataset = Dataset(x=background_train_dataset.x+train_dataset.x, 
                                    y=background_train_dataset.y+train_dataset.y, 
                                    batch_size=args.batch_size)
            print(len(train_dataset.x))

            dataset_name = args.dataset.split('/')[-1].split('.')[0]
            model_name = args.model.split('/')[-1]
            parameters_path = f"{args.cache_path}/{dataset_name}/{model_name}"

            if model.model_type == "Seq2SeqLM":
                density_based_ue = [
                    MahalanobisDistanceSeq("encoder", parameters_path=parameters_path),
                    MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    #RelativeMahalanobisDistanceSeq("encoder", parameters_path=parameters_path),
                    #RelativeMahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RDESeq("encoder", parameters_path=parameters_path),
                    RDESeq("decoder", parameters_path=parameters_path),
                    PPLMDSeq("encoder", md_type="MD", parameters_path=parameters_path),
                    #PPLMDSeq("encoder", md_type="RMD", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                    #PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
                ]
            else:
                density_based_ue = [
                    MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    #RelativeMahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RDESeq("decoder", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                    #PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
                ]
        else:
            train_dataset = None
            background_train_dataset = None
            density_based_ue = []

        if args.subsample_eval_dataset != -1:
            dataset.subsample(args.subsample_eval_dataset, seed=seed)
        
        
        man = UEManager(
            dataset,
            model,
            [
                MaximumSequenceProbability(),
            ] + density_based_ue,
            [
                RougeMetric('rougeL'),
            ],
            [
                RiskCoverageCurveAUC(),
            ],
            [
                Logger(),
            ],
            train_data=train_dataset,
            ignore_exceptions=args.ignore_exceptions,
            background_train_data=background_train_dataset,
        )
        man()
        man.save(args.save_path + f'_seed{seed}')