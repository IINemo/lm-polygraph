#!/usr/bin/env python3

import hydra
import importlib
import os
import torch
import transformers
import argparse
from pathlib import Path
import json

import logging

log = logging.getLogger()


from lm_polygraph.utils.manager import UEManager
from lm_polygraph.utils.dataset import Dataset
from lm_polygraph.utils.model import WhiteboxModel
from lm_polygraph.utils.processor import Logger
from lm_polygraph.generation_metrics import (
    RougeMetric,
    WERTokenwiseMetric,
    BartScoreSeqMetric,
    ModelScoreSeqMetric,
    ModelScoreTokenwiseMetric, 
    AccuracyMetric
)
from lm_polygraph.estimators import *
from lm_polygraph.ue_metrics import (
    ReversedPairsProportion,
    PredictionRejectionArea,
    RiskCoverageCurveAUC,
)

DENSITY_BASED_ESTIMATORS = [
    "MahalanobisDistanceSeq",
    "RelativeMahalanobisDistanceSeq",
    "RDESeq",
    "PPLMDSeq",
]


def get_args():
    parser = argparse.ArgumentParser()

    parser.add_argument("--estimator", type=str, required=True)
    parser.add_argument("--estimator_kwargs", type=json.loads, required=True)
    parser.add_argument("--estimator_module", type=str, default=None)
    
    parser.add_argument("--additional_estimators", type=json.loads, default={})

    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--train_dataset", type=str)
    parser.add_argument("--background_train_dataset", type=str)
    parser.add_argument("--text_column", type=str, default="question")
    parser.add_argument("--label_column", type=str, default="answer")
    parser.add_argument("--prompt", type=str, default='')
    parser.add_argument("--train_split", type=str, default="train")
    parser.add_argument("--eval_split", type=str, default="test")
    parser.add_argument("--max_new_tokens", type=int, default=256)

    parser.add_argument("--model", type=str, required=True)
    parser.add_argument(
        "--ignore_exceptions",
        type=bool,
        default=True,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument(
        "--use_density_based_ue",
        type=bool,
        default=False,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument(
        "--use_seq_ue",
        type=bool,
        default=True,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument(
        "--use_tok_ue",
        type=bool,
        default=False,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument(
        "--load_from_disk",
        type=bool,
        default=False,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument(
        "--train_test_split",
        type=bool,
        default=False,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument("--test_split_size", type=float, default=1)
    
    parser.add_argument("--background_train_dataset", type=str, default="allenai/c4")
    parser.add_argument("--background_train_dataset_text_column", type=str, default="text")
    parser.add_argument("--background_train_dataset_label_column", type=str, default="url")
    parser.add_argument("--background_train_dataset_data_files", type=str, default="en/c4-train.00000-of-01024.json.gz")

    parser.add_argument("--subsample_background_train_dataset", type=int, default=-1)
    parser.add_argument("--subsample_train_dataset", type=int, default=-1)
    parser.add_argument("--subsample_eval_dataset", type=int, default=-1)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--seed", nargs="+", type=int)
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--save_path", type=str, default="")

    args = parser.parse_args()

    return args

def get_density_based_ue_methods(args, model_type):
    estimators = []
    if args.use_density_based_ue:
        dataset_name = args.dataset if isinstance(args.dataset, str) else '_'.join(args.dataset)
        dataset_name = dataset_name.split("/")[-1].split(".")[0]
        model_name = args.model.split("/")[-1]
        parameters_path = f"{args.cache_path}/density_stats/{dataset_name}/{model_name}"
            
        if model_type == "Seq2SeqLM":
            estimators += [
                MahalanobisDistanceSeq("encoder", parameters_path=parameters_path),
                MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                RelativeMahalanobisDistanceSeq(
                    "encoder", parameters_path=parameters_path
                ),
                RelativeMahalanobisDistanceSeq(
                    "decoder", parameters_path=parameters_path
                ),
                RDESeq("encoder", parameters_path=parameters_path),
                RDESeq("decoder", parameters_path=parameters_path),
                PPLMDSeq("encoder", md_type="MD", parameters_path=parameters_path),
                PPLMDSeq("encoder", md_type="RMD", parameters_path=parameters_path),
                PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
            ]
        else:
            estimators += [
                MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                RelativeMahalanobisDistanceSeq(
                    "decoder", parameters_path=parameters_path
                ),
                RDESeq("decoder", parameters_path=parameters_path),
                PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
            ]
    return estimators

def get_ue_methods(args, model):
    estimators = []
    if args.use_seq_ue:
        estimators += [
                MaximumSequenceProbability(),
                Perplexity(),
                MeanTokenEntropy(),
                MeanPointwiseMutualInformation(),
                MeanConditionalPointwiseMutualInformation(tau=0.0656, lambd=3.599),
                PTrue(),
                PTrueSampling(),
                MonteCarloSequenceEntropy(),
                MonteCarloNormalizedSequenceEntropy(),
                LexicalSimilarity(metric="rouge1"),
                LexicalSimilarity(metric="rouge2"),
                LexicalSimilarity(metric="rougeL"),
                LexicalSimilarity(metric="BLEU"),
                NumSemSets(),
                EigValLaplacian(similarity_score="NLI_score", affinity="entail"),
                EigValLaplacian(similarity_score="NLI_score", affinity="contra"),
                EigValLaplacian(similarity_score="Jaccard_score"),
                DegMat(similarity_score="NLI_score", affinity="entail"),
                DegMat(similarity_score="NLI_score", affinity="contra"),
                DegMat(similarity_score="Jaccard_score"),
                Eccentricity(similarity_score="NLI_score", affinity="entail"),
                Eccentricity(similarity_score="NLI_score", affinity="contra"),
                Eccentricity(similarity_score="Jaccard_score"),
                SemanticEntropy(),
            ]
    if args.use_tok_ue:
        estimators += [
                MaximumTokenProbability(),
                TokenEntropy(),
                PointwiseMutualInformation(),
                ConditionalPointwiseMutualInformation(
                    tau=0.0656, lambd=3.599
                ),  # TODO: What is this ????
                SemanticEntropyToken(model.model_path, args.cache_path),
            ]
    return estimators

def main_run(config, save_path):
    args = get_args() if config is None else config
    save_path = args.save_path if "save_path" in args else save_path
    estimators = []

    if args.estimator is not None:
        if args.estimator_module is None:
            estimator_class = globals()[args.estimator]
        else:
            module = importlib.import_module(args.estimator_module)
            estimator_class = getattr(module, args.estimator)
        estimators.append(estimator_class(**args.estimator_kwargs))
        
    if args.seed is None or len(args.seed) == 0:
        args.seed = [1]

    device = args.device
    if device is None:
        device = "cuda:0" if torch.cuda.device_count() > 0 else "cpu"

    for seed in args.seed:
        log.info("=" * 100)
        log.info(f"SEED: {seed}")

        log.info(f"Loading model {args.model}...")
        transformers.set_seed(seed)
        model = WhiteboxModel.from_pretrained(
            args.model,
            device=device,
        )
        log.info("Done with loading model.")

        log.info(f"Loading dataset {args.dataset}...")
        dataset = Dataset.load(
            args.dataset,
            args.text_column,
            args.label_column,
            batch_size=args.batch_size,
            prompt=args.prompt,
            split=args.eval_split,
            load_from_disk=args.load_from_disk
        )
        
        estimators += get_ue_methods(args, model)
        estimators += get_density_based_ue_methods(args, model.model_type)

        if any([str(estimator).split('_')[0] in DENSITY_BASED_ESTIMATORS for estimator in estimators]):
            if (args.train_dataset is not None) and (
                args.train_dataset != args.dataset
            ):
                train_dataset = Dataset.load(
                    args.train_dataset,
                    args.text_column,
                    args.label_column,
                    batch_size=args.batch_size,
                    prompt=args.prompt,
                    split=args.train_split,
                    size=10_000,
                    load_from_disk=args.load_from_disk
                )
            elif args.train_test_split:
                X_train, X_test, y_train, y_test = dataset.train_test_split(
                    test_size=args.test_split_size, seed=seed, split=args.eval_split
                )
                train_dataset = Dataset(
                    x=X_train, y=y_train, batch_size=args.batch_size
                )
            else:
                train_dataset = Dataset.load(
                    args.dataset,
                    args.text_column,
                    args.label_column,
                    batch_size=args.batch_size,
                    prompt=args.prompt,
                    split=args.train_split,
                    size=10_000,
                    load_from_disk=args.load_from_disk,
                )
                   
            background_train_dataset = Dataset.load(
                    args.background_train_dataset,
                    args.background_train_dataset_text_column, 
                    args.background_train_dataset_label_column,
                    batch_size=args.batch_size,
                    data_files=args.background_train_dataset_data_files,
                    split="train",
                    size=100_000,
            )

            if args.subsample_train_dataset != -1:
                train_dataset.subsample(args.subsample_train_dataset, seed=seed)
            if args.subsample_background_train_dataset != -1:
                background_train_dataset.subsample(
                    args.subsample_background_train_dataset, seed=seed
                )

            log.info("Done with loading data.")
        else:
            train_dataset = None
            background_train_dataset = None
            density_based_ue = []

        if args.subsample_eval_dataset != -1:
            dataset.subsample(args.subsample_eval_dataset, seed=seed)

        man = UEManager(
            dataset,
            model,
            estimators,
            [
                RougeMetric("rouge1"),
                RougeMetric("rouge2"),
                RougeMetric("rougeL"),
                BartScoreSeqMetric("rh"),
                ModelScoreSeqMetric("model_rh"),
                AccuracyMetric(),
                ModelScoreTokenwiseMetric("model_rh"),
                WERTokenwiseMetric(),
            ],
            [
                ReversedPairsProportion(),
                PredictionRejectionArea(),
                RiskCoverageCurveAUC(),
            ],
            [
                Logger(),
            ],
            train_data=train_dataset,
            ignore_exceptions=args.ignore_exceptions,
            background_train_data=background_train_dataset,
            max_new_tokens=args.max_new_tokens,
        )

        man()

        man.save(save_path + f"/ue_manager_seed{seed}")


if "HYDRA_CONFIG" in os.environ:
    hydra_config = Path(os.environ["HYDRA_CONFIG"])

    @hydra.main(
        version_base=None,
        config_path=str(hydra_config.parent),
        config_name=str(hydra_config.name),
    )
    def main_hydra(config):
        output_dir = os.getcwd()
        log.info(f"Main directory: {output_dir}")
        os.chdir(hydra.utils.get_original_cwd())
        main_run(config, output_dir)

    main = main_hydra

else:
    main = lambda: main_run(None, None)


if __name__ == "__main__":
    main()
