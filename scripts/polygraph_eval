#!/usr/bin/env python3

import os
import copy
import os
import torch
import transformers
import argparse

from lm_polygraph.utils.manager import UEManager
from lm_polygraph.utils.dataset import Dataset
from lm_polygraph.utils.model import WhiteboxModel
from lm_polygraph.utils.processor import Logger
from lm_polygraph.generation_metrics import RougeMetric, WERTokenwiseMetric, BartScoreSeqMetric, ModelScoreSeqMetric, ModelScoreTokenwiseMetric
from lm_polygraph.estimators import *
from lm_polygraph.ue_metrics import ReversedPairsProportion, PredictionRejectionArea, RiskCoverageCurveAUC


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str,
                        default='/Users/ekaterinafadeeva/work/data/uncertainty_datasets/triviaqa.csv')
    parser.add_argument("--train_dataset", type=str, default=None)
    parser.add_argument("--background_train_dataset", type=str, default="allenai/c4")
    parser.add_argument("--model", type=str, default='databricks/dolly-v2-3b')
    parser.add_argument("--use_density_based_ue", type=bool, default=True, action=argparse.BooleanOptionalAction)
    parser.add_argument("--ignore_exceptions", type=bool, default=True, action=argparse.BooleanOptionalAction)
    parser.add_argument("--subsample_background_train_dataset", type=int, default=1000)
    parser.add_argument("--subsample_train_dataset", type=int, default=100)
    parser.add_argument("--subsample_eval_dataset", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--seed", nargs='+', type=int)
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--save_path", type=str,
                        default='/home/petrakov/emnlp/lm-polygraph/workdir/save')
    parser.add_argument("--cache_path", type=str,
                        default='/home/petrakov/emnlp/lm-polygraph/workdir')
    args = parser.parse_args()

    if args.seed is None or len(args.seed) == 0:
        args.seed = [1]

    device = args.device
    if device is None:
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

    for seed in args.seed:
        print('=' * 150)
        print('SEED:', seed)
        transformers.set_seed(seed)
        model = WhiteboxModel.from_pretrained(
            args.model,
            device=device,
        )

        dataset = Dataset.load(
            args.dataset,
            'question', 'answer',
            batch_size=args.batch_size,
        )

        if args.use_density_based_ue:
            if (args.train_dataset is not None) and (args.train_dataset != args.dataset):
                train_dataset = Dataset.load(
                    args.train_dataset,
                    'question', 'answer',
                    batch_size=args.batch_size,
                 )
            else:
                X_train, X_test, y_train, y_test = dataset.train_test_split(
                    test_size=0.7,
                    seed=seed,
                    split="eval"
                )
                train_dataset = Dataset(x=X_train, y=y_train, batch_size=args.batch_size)
            background_train_dataset = Dataset.load(
                    args.background_train_dataset,
                    'text', 'url',
                    batch_size=args.batch_size,
                    data={"data_files": "en/c4-train.00000-of-01024.json.gz"},
                    max_size=100_000,
                    split="train",
            )

            if args.subsample_train_dataset != -1:
                train_dataset.subsample(args.subsample_train_dataset, seed=seed)
            if args.subsample_background_train_dataset != -1:
                background_train_dataset.subsample(args.subsample_background_train_dataset, seed=seed)

            dataset_name = args.dataset.split('/')[-1].split('.')[0]
            model_name = args.model.split('/')[-1]
            parameters_path = f"{args.cache_path}/{dataset_name}/{model_name}"

            if model.model_type == "Seq2SeqLM":
                density_based_ue = [
                    MahalanobisDistanceSeq("encoder", parameters_path=parameters_path),
                    MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RelativeMahalanobisDistanceSeq("encoder", parameters_path=parameters_path),
                    RelativeMahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RDESeq("encoder", parameters_path=parameters_path),
                    RDESeq("decoder", parameters_path=parameters_path),
                    PPLMDSeq("encoder", md_type="MD", parameters_path=parameters_path),
                    PPLMDSeq("encoder", md_type="RMD", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
                ]
            else:
                density_based_ue = [
                    MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RelativeMahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RDESeq("decoder", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
                ]
        else:
            train_dataset = None
            background_train_dataset = None
            density_based_ue = []

        if args.subsample_eval_dataset != -1:
            dataset.subsample(args.subsample_eval_dataset, seed=seed)
        
        
        man = UEManager(
            dataset,
            model,
            [
                MaximumSequenceProbability(),
                Perplexity(),
                MeanTokenEntropy(),
                MeanPointwiseMutualInformation(),
                MeanConditionalPointwiseMutualInformation(tau=0.0656, lambd=3.599),
                PTrue(),
                PTrueSampling(),
                MonteCarloSequenceEntropy(),
                MonteCarloNormalizedSequenceEntropy(),
                LexicalSimilarity(metric='rouge1'),
                LexicalSimilarity(metric='rouge2'),
                LexicalSimilarity(metric='rougeL'),
                LexicalSimilarity(metric='BLEU'),
                NumSemSets(),
                EigValLaplacian(similarity_score='NLI_score', affinity='entail'),
                EigValLaplacian(similarity_score='NLI_score', affinity='contra'),
                EigValLaplacian(similarity_score='Jaccard_score'),
                DegMat(similarity_score='NLI_score', affinity='entail'),
                DegMat(similarity_score='NLI_score', affinity='contra'),
                DegMat(similarity_score='Jaccard_score'),
                Eccentricity(similarity_score='NLI_score', affinity='entail'),
                Eccentricity(similarity_score='NLI_score', affinity='contra'),
                Eccentricity(similarity_score='Jaccard_score'),
                SemanticEntropy(),

                MaximumTokenProbability(),
                TokenEntropy(),
                PointwiseMutualInformation(),
                ConditionalPointwiseMutualInformation(tau=0.0656, lambd=3.599),
                SemanticEntropyToken(model.model_path, args.cache_path),
            ] + density_based_ue,
            [
                RougeMetric('rouge1'),
                RougeMetric('rouge2'),
                RougeMetric('rougeL'),
                BartScoreSeqMetric('rh'),
                ModelScoreSeqMetric('model_rh'),
                ModelScoreTokenwiseMetric('model_rh'),
                WERTokenwiseMetric(),
            ],
            [
                ReversedPairsProportion(),
                PredictionRejectionArea(),
                RiskCoverageCurveAUC(),
            ],
            [
                Logger(),
            ],
            train_data=train_dataset,
            ignore_exceptions=args.ignore_exceptions,
            background_train_data=background_train_dataset,
        )
        man()
        man.save(args.save_path + f'_seed{seed}')