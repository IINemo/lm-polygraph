#!/usr/bin/env python3

import hydra
import importlib
import os
import torch
import transformers
import argparse
from pathlib import Path
import json

import logging

log = logging.getLogger()


from lm_polygraph.utils.manager import UEManager
from lm_polygraph.utils.dataset import Dataset
from lm_polygraph.utils.model import WhiteboxModel
from lm_polygraph.utils.processor import Logger
from lm_polygraph.generation_metrics import (
    RougeMetric,
    WERTokenwiseMetric,
    BartScoreSeqMetric,
    ModelScoreSeqMetric,
    ModelScoreTokenwiseMetric,
)
from lm_polygraph.estimators import *
from lm_polygraph.ue_metrics import (
    ReversedPairsProportion,
    PredictionRejectionArea,
    RiskCoverageCurveAUC,
)


DENSITY_BASED_ESTIMATORS = [
    "MahalanobisDistanceSeq",
    "RelativeMahalanobisDistanceSeq",
    "RDESeq",
    "PPLMDSeq",
]


def get_args():
    parser = argparse.ArgumentParser()

    parser.add_argument("--estimator", type=str, required=True)
    parser.add_argument("--estimator_kwargs", type=json.loads, required=True)
    parser.add_argument("--estimator_module", type=str, default=None)

    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--train_dataset", type=str)
    parser.add_argument("--background_train_dataset", type=str)
    parser.add_argument("--text_column", type=str, default="question")
    parser.add_argument("--label_column", type=str, default="answer")

    parser.add_argument("--model", type=str, required=True)
    parser.add_argument(
        "--ignore_exceptions",
        type=bool,
        default=True,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument("--subsample_background_train_dataset", type=int, default=-1)
    parser.add_argument("--subsample_train_dataset", type=int, default=-1)
    parser.add_argument("--subsample_eval_dataset", type=int, default=-1)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--seed", nargs="+", type=int)
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--save_path", type=str, default="")
    parser.add_argument("--cache_path", type=str, default="")

    args = parser.parse_args()

    return args


def main_run(config, save_path):
    args = get_args() if config is None else config
    save_path = args.save_path if "save_path" in args else save_path

    if args.estimator_module is None:
        estimator_class = globals()[args.estimator]
    else:
        module = importlib.import_module(module_name)
        estimator_class = getattr(module, args.estimator)
    estimator = estimator_class(**args.estimator_kwargs)

    if args.seed is None or len(args.seed) == 0:
        args.seed = [1]

    device = args.device
    if device is None:
        device = "cuda:0" if torch.cuda.device_count() > 0 else "cpu"

    for seed in args.seed:
        log.info("=" * 100)
        log.info(f"SEED: {seed}")

        log.info(f"Loading model {args.model}...")
        transformers.set_seed(seed)
        model = WhiteboxModel.from_pretrained(
            args.model,
            device=device,
        )
        log.info("Done with loading model.")

        log.info(f"Loading dataset {args.dataset}...")
        dataset = Dataset.load(
            args.dataset,
            args.text_column,
            args.label_column,
            batch_size=args.batch_size,
        )

        if args.estimator in DENSITY_BASED_ESTIMATORS:
            if (args.train_dataset is not None) and (
                args.train_dataset != args.dataset
            ):
                train_dataset = Dataset.load(
                    args.train_dataset,
                    args.text_column,
                    args.label_column,
                    batch_size=args.batch_size,
                )
            else:
                X_train, X_test, y_train, y_test = dataset.train_test_split(
                    test_size=0.7, seed=seed, split="eval"  # TODO: !
                )
                train_dataset = Dataset(
                    x=X_train, y=y_train, batch_size=args.batch_size
                )
                
            background_train_dataset = Dataset.load(
                args.background_train_dataset,
                "text",
                "url",
                batch_size=args.batch_size,
                data={
                    "data_files": "en/c4-train.00000-of-01024.json.gz"
                },  # TODO: What is this ????
                max_size=100_000,  # TODO:!
                split="train",
            )

            if args.subsample_train_dataset != -1:
                train_dataset.subsample(args.subsample_train_dataset, seed=seed)
            if args.subsample_background_train_dataset != -1:
                background_train_dataset.subsample(
                    args.subsample_background_train_dataset, seed=seed
                )

            log.info("Done with loading data.")

            dataset_name = args.dataset.split("/")[-1].split(".")[0]
            model_name = args.model.split("/")[-1]
            parameters_path = f"{args.cache_path}/{dataset_name}/{model_name}"
        else:
            train_dataset = None
            background_train_dataset = None
            density_based_ue = []

        if args.subsample_eval_dataset != -1:
            dataset.subsample(args.subsample_eval_dataset, seed=seed)

        man = UEManager(
            dataset,
            model,
            [
                estimator,
            ],
            [
                RougeMetric("rouge1"),
                RougeMetric("rouge2"),
                RougeMetric("rougeL"),
                BartScoreSeqMetric("rh"),
                ModelScoreSeqMetric("model_rh"),
                ModelScoreTokenwiseMetric("model_rh"),
                WERTokenwiseMetric(),
            ],
            [
                ReversedPairsProportion(),
                PredictionRejectionArea(),
                RiskCoverageCurveAUC(),
            ],
            [
                Logger(),
            ],
            train_data=train_dataset,
            ignore_exceptions=args.ignore_exceptions,
            background_train_data=background_train_dataset,
        )

        man()

        man.save(save_path + f"_seed{seed}")


if "HYDRA_CONFIG" in os.environ:
    hydra_config = Path(os.environ["HYDRA_CONFIG"])

    @hydra.main(
        version_base=None,
        config_path=str(hydra_config.parent),
        config_name=str(hydra_config.name),
    )
    def main_hydra(config):
        output_dir = os.getcwd()
        print(output_dir)
        os.chdir(hydra.utils.get_original_cwd())
        main_run(config, output_dir)

    main = main_hydra

else:
    main = lambda: main_run(None, None)


if __name__ == "__main__":
    main()
