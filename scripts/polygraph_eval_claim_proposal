#!/usr/bin/env python3

import hydra
import os
import transformers
from pathlib import Path

from lm_polygraph.utils.manager import UEManager
from lm_polygraph.utils.dataset import Dataset
from lm_polygraph.utils.model import WhiteboxModel, create_ensemble
from lm_polygraph.utils.processor import Logger
from lm_polygraph.generation_metrics import *
from lm_polygraph.estimators import *
from lm_polygraph.estimators.ensemble_token_measures import *
from lm_polygraph.ue_metrics import *
from lm_polygraph.utils.common import load_external_module
from lm_polygraph.utils.generation_parameters import GenerationParameters
from lm_polygraph.utils.builder_enviroment_stat_calculator import (
    BuilderEnvironmentStatCalculator,
    StatCalculatorContainer,
)
from lm_polygraph.defaults.register_default_stat_calculators import (
    register_default_stat_calculators,
)
from lm_polygraph.utils.factory_estimator import FactoryEstimator

import logging

log = logging.getLogger("lm_polygraph")

hydra_config = Path(os.environ["HYDRA_CONFIG"])


@hydra.main(
    version_base=None,
    config_path=str(hydra_config.parent),
    config_name=str(hydra_config.name),
)
def main(args):
    save_path = os.getcwd()
    log.info(f"Main directory: {save_path}")
    os.chdir(hydra.utils.get_original_cwd())

    save_path = args.save_path if "save_path" in args else save_path

    if args.seed is None or len(args.seed) == 0:
        args.seed = [1]

    cache_kwargs = {}
    if os.environ.get("HF_DATASETS_OFFLINE", "").strip() == "1":
        cache_kwargs = {"cache_dir": args.cache_path}

    for seed in args.seed:
        log.info("=" * 100)
        log.info(f"SEED: {seed}")

        log.info(f"Loading model {args.model.path}...")
        transformers.set_seed(seed)

        model = get_model(args)

        if args.model.ensemble:
            # Only MC-ensembles for now
            log.info(f"Creating ensemble...")
            base_model = get_model(args)
            ensemble_model = create_ensemble(
                models=[base_model],
                mc=True,
                seed=args.seed[0],
                ensembling_mode=args.model.ensembling_mode,
                mc_seeds=args.model.mc_seeds,
                dropout_rate=float(args.model.dropout_rate),
                **cache_kwargs,
            )
        else:
            ensemble_model = None

        log.info("Done with loading model.")

        log.info(f"Loading dataset {args.dataset}...")
        dataset = Dataset.load(
            args.dataset,
            args.text_column,
            getattr(args, "label_column", None),
            batch_size=args.batch_size,
            prompt=getattr(args, "prompt", ""),
            description=getattr(args, "description", ""),
            mmlu_max_subject_size=getattr(args, "mmlu_max_subject_size", 100),
            n_shot=getattr(args, "n_shot", 5),
            few_shot_split=getattr(args, "few_shot_split", "train"),
            split=args.eval_split,
            load_from_disk=args.load_from_disk,
            **cache_kwargs,
        )
        log.info("Done with loading eval data.")

        log.info("=" * 100)
        log.info("Initializing UE estimators...")
        estimators = []
        estimators += get_ue_methods(args)
        log.info("Done loading UE estimators")

        train_dataset = None
        background_train_dataset = None

        if args.subsample_eval_dataset != -1:
            dataset.subsample(args.subsample_eval_dataset, seed=seed)

        generation_metrics = get_generation_metrics(args)

        ue_metrics = get_ue_metrics(args)

        builder_stat_calculators = BuilderEnvironmentStatCalculator(model=model)
        available_stat_calculators = get_stat_calculator_names(args)

        man = UEManager(
            dataset,
            model,
            estimators,
            builder_stat_calculators,
            available_stat_calculators,
            generation_metrics,
            ue_metrics,
            [
                Logger(),
            ],
            train_data=train_dataset,
            ignore_exceptions=args.ignore_exceptions,
            background_train_data=background_train_dataset,
            max_new_tokens=args.max_new_tokens,
            ensemble_model=ensemble_model,
            cache_path=args.cache_path,
        )

        man()

        man.save(save_path + f"/ue_manager_seed{seed}")


def get_ue_metrics(args):
    ue_metrics = [
        ReversedPairsProportion(),
        PredictionRejectionArea(),
        RiskCoverageCurveAUC(),
    ]
    if getattr(args, "use_claim_ue", False):
        ue_metrics += [
            ROCAUC(),
            PRAUC(),
        ]
    return ue_metrics


def get_ue_methods(config):
    result_estimators = []
    factory = FactoryEstimator()
    for estimator in config.estimators:
        result_estimators.append(
            factory(estimator.name, estimator.cfg if "cfg" in estimator else dict())
        )

    return result_estimators


def get_stat_calculator_names(config):
    result_stat_calculators = dict()
    result_stat_dependencies = dict()

    for stat_calculator in config.stat_calculators:
        if stat_calculator == "auto":
            sc, sd = register_default_stat_calculators()
            sc = {k: StatCalculatorContainer(obj=v) for k, v in sc.items()}
            result_stat_calculators.update(sc)
            result_stat_dependencies.update(sd)
        else:
            result_stat_calculators.update(
                {
                    s: StatCalculatorContainer(
                        name=stat_calculator.name,
                        cfg=stat_calculator.cfg,
                        stats=stat_calculator.stats,
                        dependencies=stat_calculator.dependencies,
                        builder=stat_calculator.builder,
                    )
                    for s in stat_calculator.stats
                }
            )
            result_stat_dependencies.update(
                {
                    s: (
                        stat_calculator.dependencies
                        if stat_calculator.dependencies is not None
                        else []
                    )
                    for s in stat_calculator.stats
                }
            )

    return result_stat_calculators, result_stat_dependencies


def get_generation_metrics(args):
    log.info("=" * 100)
    log.info("Initializing generation metrics...")

    generation_metrics = getattr(args, "generation_metrics", None)
    if not generation_metrics:
        result = [
            RougeMetric("rouge1"),
            RougeMetric("rouge2"),
            RougeMetric("rougeL"),
            BLEUMetric(),
            BertScoreMetric("rh"),
            SbertMetric(),
            AccuracyMetric(
                target_ignore_regex=getattr(args, "target_ignore_regex", None),
                output_ignore_regex=getattr(args, "output_ignore_regex", None),
                normalize=getattr(args, "normalize", False),
            ),
            AlignScore(),
        ]
        if getattr(args, "use_claim_ue", False):
            result += [OpenAIFactCheck(cache_path=args.cache_path)]
        if args.task == "nmt":
            ignore_regex = getattr(args, "source_ignore_regex", None)
            result += [Comet(source_ignore_regex=ignore_regex)]
        if not getattr(args, "multiref", False):
            # Currently, BartScoreSeqMetric does not support multiref
            result.append(BartScoreSeqMetric("rh"))
        else:
            # Wrap each metric in AggregatedMetric
            result = [AggregatedMetric(base_metric=metric) for metric in result]
    else:
        result = []
        for metric in generation_metrics:
            metric_name = metric["name"]
            if getattr(args, "multiref", False) and metric_name == "BartScoreSeqMetric":
                raise ValueError("BartScoreSeqMetric does not support multiref")
            metric_class = globals()[metric_name]
            result.append(metric_class(*metric.get("args", [])))

    log.info("Done with initializing generation metrics.")

    return result


def get_model(args, cache_kwargs={}):
    if not "path_to_load_script" in args.model:
        log.warning(
            "Loading model by directly passing the path to the model is deprecated and will be removed in the next release. Please use loading script instead."
        )
        return WhiteboxModel.from_pretrained(
            args.model.path,
            getattr(args, "generation_params", {}),
            device_map=args.model.device_map,
            **cache_kwargs,
        )

    path_to_load_script = Path(args.model.path_to_load_script)
    if not os.path.isabs(path_to_load_script):
        path_to_load_script = hydra_config.parent / path_to_load_script

    load_module = load_external_module(path_to_load_script)

    load_model_args = {"model_path": args.model.path}
    load_model_args.update(args.model.load_model_args)
    base_model = load_module.load_model(**load_model_args)

    load_tok_args = {"model_path": args.model.path}
    load_tok_args.update(args.model.load_tokenizer_args)
    tokenizer = load_module.load_tokenizer(**load_tok_args)

    generation_params = GenerationParameters(**getattr(args, "generation_params", {}))

    model = WhiteboxModel(
        base_model, tokenizer, args.model.path, args.model.type, generation_params
    )

    return model


if __name__ == "__main__":
    main()
