from typing import List, Union
from dataclasses import dataclass

from lm_polygraph.utils.model import Model, WhiteboxModel
from lm_polygraph.model_adapters.visual_whitebox_model import VisualWhiteboxModel
from lm_polygraph.estimators.estimator import Estimator
from lm_polygraph.utils.manager import UEManager
from lm_polygraph.utils.dataset import Dataset
from lm_polygraph.utils.builder_enviroment_stat_calculator import (
    BuilderEnvironmentStatCalculator,
)
from lm_polygraph.defaults.register_default_stat_calculators import (
    register_default_stat_calculators,
)


@dataclass
class UncertaintyOutput:
    """
    Uncertainty estimator output.

    Parameters:
        uncertainty (float): uncertainty estimation.
        input_text (str): text used as model input.
        generation_text (str): text generated by the model.
        model_path (str): path to the model used in generation.
    """

    uncertainty: Union[float, List[float]]
    input_text: str
    generation_text: str
    generation_tokens: List[int]
    model_path: str
    estimator: str


def estimate_uncertainty(
    model: Model, estimator: Estimator, input_text: str
) -> UncertaintyOutput:
    """
    Estimate uncertainty of model generation for a single input text.
    
    This is the primary high-level function for uncertainty estimation in LM-Polygraph.
    It handles the complete pipeline: generation, statistics calculation, and uncertainty
    estimation, returning a structured output with the uncertainty score and metadata.
    
    The function automatically:
    - Generates text from the model based on the input
    - Calculates required statistics for the chosen estimator
    - Applies the uncertainty estimation method
    - Packages results in a structured format
    
    Parameters:
        model: Language model to evaluate. Can be either:
            - WhiteboxModel: For methods requiring access to logits/internals
            - BlackboxModel: For methods that work with text outputs only
        estimator: Uncertainty estimation method to apply. Choose from:
            - Information-based: Perplexity, TokenEntropy, MaximumSequenceProbability
            - Meaning diversity: SemanticEntropy, LexicalSimilarity, NumSemSets
            - Density-based: MahalanobisDistance (requires training data)
            - Reflexive: PTrue (asks model about its confidence)
            See lm_polygraph.estimators for full list
        input_text: Single input prompt/question to evaluate
        
    Returns:
        UncertaintyOutput: Structured output containing:
            - uncertainty: Float or list of floats with uncertainty scores
              (higher = more uncertain)
            - input_text: The original input text
            - generation_text: Text generated by the model
            - generation_tokens: Token IDs of the generation
            - model_path: Path/name of the model used
            - estimator: Name of the uncertainty estimator
            
    Raises:
        ValueError: If estimator is incompatible with model type
        RuntimeError: If generation or estimation fails
        
    Examples:
        Basic usage with whitebox model:
        >>> from lm_polygraph import WhiteboxModel
        >>> from lm_polygraph.estimators import TokenEntropy
        >>> model = WhiteboxModel.from_pretrained('gpt2')
        >>> estimator = TokenEntropy()
        >>> result = estimate_uncertainty(
        ...     model, estimator, 
        ...     "What is the capital of France?"
        ... )
        >>> print(f"Generated: {result.generation_text}")
        >>> print(f"Uncertainty: {result.uncertainty}")
        
        With blackbox model:
        >>> from lm_polygraph import BlackboxModel
        >>> from lm_polygraph.estimators import LexicalSimilarity
        >>> model = BlackboxModel.from_openai('YOUR_KEY', 'gpt-3.5-turbo')
        >>> estimator = LexicalSimilarity()
        >>> result = estimate_uncertainty(
        ...     model, estimator,
        ...     "Explain quantum entanglement"
        ... )
        
        Token-level uncertainty:
        >>> from lm_polygraph.estimators import TokenEntropy
        >>> estimator = TokenEntropy()  # Returns uncertainty per token
        >>> result = estimate_uncertainty(model, estimator, "Hello world")
        >>> # result.uncertainty contains list of values per token
        
    See Also:
        estimate_uncertainty_batched: For processing multiple inputs efficiently
        UEManager: Low-level API for custom uncertainty estimation pipelines
    """
    # model_type = "Whitebox" if isinstance(model, WhiteboxModel) else "Blackbox"
    if isinstance(model, WhiteboxModel):
        model_type = "Whitebox"
    elif isinstance(model, VisualWhiteboxModel):
        model_type = "VisualLM"
    else:
        model_type = "Blackbox"
    man = UEManager(
        Dataset([input_text], [""], batch_size=1),
        model,
        [estimator],
        available_stat_calculators=register_default_stat_calculators(
            model_type
        ),  # TODO:
        builder_env_stat_calc=BuilderEnvironmentStatCalculator(model),
        generation_metrics=[],
        ue_metrics=[],
        processors=[],
        ignore_exceptions=False,
        verbose=False,
    )
    man()
    ue = man.estimations[estimator.level, str(estimator)]
    texts = man.stats.get("greedy_texts", None)
    tokens = man.stats.get("greedy_tokens", None)
    if tokens is not None and len(tokens) > 0:
        # Remove last token, which is the end of the sequence token
        # since we don't include it's uncertainty in the estimator's output
        tokens = tokens[0][:-1]
    return UncertaintyOutput(
        ue[0], input_text, texts[0], tokens, model.model_path, str(estimator)
    )
