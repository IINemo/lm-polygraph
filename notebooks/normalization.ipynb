{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc85b82b-56e0-4b27-8074-1a3e06df81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = !pwd\n",
    "pwd = pwd[0]\n",
    "\n",
    "# Absolute path to default Hydra config for normalization script\n",
    "config_path = '/'.join(pwd.split('/')[:-1]) + '/examples/configs/normalization/fit/default.yaml'\n",
    "\n",
    "# Absolute path to saved train and eval managers\n",
    "EVAL_MAN_PATHS = {\n",
    "  'CoQA': pwd + '/polygraph_tacl_stablelm12b_coqa.man',\n",
    "  'GSM8K': pwd + '/polygraph_tacl_stablelm12b_gsm8k.man',\n",
    "  'MMLU': pwd + '/polygraph_tacl_stablelm12b_mmlu.man',\n",
    "  'TriviaQA': pwd + '/polygraph_tacl_stablelm12b_triviaqa.man',\n",
    "  'WMT14': pwd + '/polygraph_tacl_stablelm12b_wmt14.man',\n",
    "  'WMT19': pwd + '/polygraph_tacl_stablelm12b_wmt19.man',\n",
    "  'XSum': pwd + '/polygraph_tacl_stablelm12b_xsum.man',\n",
    "}\n",
    "\n",
    "TRAIN_MAN_PATHS = {\n",
    "  'CoQA': pwd + '/polygraph_tacl_stablelm12b_coqa_train.man',\n",
    "  'GSM8K': pwd + '/polygraph_tacl_stablelm12b_gsm8k_train.man',\n",
    "  'MMLU': pwd + '/polygraph_tacl_stablelm12b_mmlu_train.man',\n",
    "  'TriviaQA': pwd + '/polygraph_tacl_stablelm12b_triviaqa_train.man',\n",
    "  'WMT14': pwd + '/polygraph_tacl_stablelm12b_wmt14_train.man',\n",
    "  'WMT19': pwd + '/polygraph_tacl_stablelm12b_wmt19_train.man',\n",
    "  'XSum': pwd + '/polygraph_tacl_stablelm12b_xsum_train.man',\n",
    "}\n",
    "\n",
    "DATASET_NAMES = list(TRAIN_MAN_PATHS.keys())\n",
    "\n",
    "UE_METHOD_NAMES = [\n",
    "   'MaximumSequenceProbability',\n",
    "   'Perplexity',\n",
    "   'MeanTokenEntropy',\n",
    "   'MeanPointwiseMutualInformation',\n",
    "   'MeanConditionalPointwiseMutualInformation',\n",
    "   'PTrue',\n",
    "   'PTrueSampling',\n",
    "   'MonteCarloSequenceEntropy',\n",
    "   'MonteCarloNormalizedSequenceEntropy',\n",
    "   'LexicalSimilarity_rouge1',\n",
    "   'LexicalSimilarity_rouge2',\n",
    "   'LexicalSimilarity_rougeL',\n",
    "   'LexicalSimilarity_BLEU',\n",
    "   'NumSemSets',\n",
    "   'EigValLaplacian_NLI_score_entail',\n",
    "   'EigValLaplacian_NLI_score_contra',\n",
    "   'EigValLaplacian_Jaccard_score',\n",
    "   'DegMat_NLI_score_entail',\n",
    "   'DegMat_NLI_score_contra',\n",
    "   'DegMat_Jaccard_score',\n",
    "   'Eccentricity_NLI_score_entail',\n",
    "   'Eccentricity_NLI_score_contra',\n",
    "   'Eccentricity_Jaccard_score',\n",
    "   'SemanticEntropy',\n",
    "]\n",
    "\n",
    "GEN_METRIC_NAMES = ['AlignScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260d8fa3-c70d-451b-8de5-6f68e3870293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all managers to current directory\n",
    "# !wget -r --cut-dirs=2 -nH --no-parent -A '*man' http://209.38.249.180:8000/polygraph_data/mans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a60d0d-2255-4719-b08f-f8b3a72efa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_man_paths_list(man_paths):\n",
    "    \"\"\" Formats a list of paths so it can be passed as a parameter override to hydra script call\"\"\"\n",
    "    paths = ['\"' + path + '\"' for path in man_paths]\n",
    "    paths = '\\'[' + ','.join(paths) + ']\\''\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d90b943-ff6a-480a-bf5d-d3bd22e258d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f0862c0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a245bf10>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f0386d0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2997b23b0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f0e8460>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f0870a0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2aabfbf70>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run polygraph_normalize to fit all normalizers using all train datasets\n",
    "train_man_paths = get_man_paths_list(list(TRAIN_MAN_PATHS.values()))\n",
    "os.system(f'HYDRA_CONFIG={config_path} polygraph_normalize save_path=\"./\" man_paths={train_man_paths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d2d6d7-39f9-4bf4-842e-24f8f464d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load saved fitted normalizers\n",
    "with open('fitted_normalizers.json', 'rb') as f:\n",
    "    fitted_normalizers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3d056e-e44a-4b17-a2b1-0dcadfdf94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ue_abssum/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from lm_polygraph.normalizers.minmax import MinMaxNormalizer\n",
    "from lm_polygraph.normalizers.quantile import QuantileNormalizer\n",
    "from lm_polygraph.normalizers.binned_pcc import BinnedPCCNormalizer\n",
    "from lm_polygraph.normalizers.isotonic_pcc import IsotonicPCCNormalizer\n",
    "\n",
    "NORMALIZERS = {\n",
    "    'min_max': MinMaxNormalizer,\n",
    "    'quantile': QuantileNormalizer,\n",
    "    'binned_pcc': BinnedPCCNormalizer,\n",
    "    'isotonic_pcc': IsotonicPCCNormalizer\n",
    "}\n",
    "\n",
    "def get_confidences(normalizers, ues):\n",
    "    \"\"\" For each combination of method, gen metric and normalizer type\n",
    "    load normalizer from encoded string and use it to transform UE array \"\"\"\n",
    "    \n",
    "    confidences = {'min_max': defaultdict(dict),\n",
    "                   'quantile': defaultdict(dict),\n",
    "                   'binned_pcc': defaultdict(dict),\n",
    "                   'isotonic_pcc': defaultdict(dict)}\n",
    "    \n",
    "    for key in confidences.keys():\n",
    "        for method_name in UE_METHOD_NAMES:\n",
    "            for metric_name in GEN_METRIC_NAMES:\n",
    "                normalizer = NORMALIZERS[key].loads(normalizers[(metric_name, method_name, key)])\n",
    "                confidences[key][metric_name][method_name] = normalizer.transform(ues[method_name])\n",
    "\n",
    "    return confidences\n",
    "\n",
    "def calculate_mses(confidences, gen_metrics):\n",
    "    \"\"\" Given a list of confidences and gen metrics calculates MSE between them\n",
    "    for each combination of method, metric and normalizer type\"\"\"\n",
    "    \n",
    "    mses = {'min_max': defaultdict(dict),\n",
    "            'quantile': defaultdict(dict),\n",
    "            'binned_pcc': defaultdict(dict),\n",
    "            'isotonic_pcc': defaultdict(dict)}\n",
    "    \n",
    "    for key in confidences.keys():\n",
    "        for metric_name in GEN_METRIC_NAMES:\n",
    "            metric_mses = []\n",
    "            for method_name in UE_METHOD_NAMES:\n",
    "                gt_metric = gen_metrics[metric_name]\n",
    "                mse = ((confidences[key][metric_name][method_name] - gt_metric) ** 2).mean()\n",
    "                metric_mses.append(mse)\n",
    "            mses[key][metric_name] = metric_mses\n",
    "\n",
    "    return mses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a1cf6-e03f-439b-b246-428ffefcbb68",
   "metadata": {},
   "source": [
    "### All datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ae19fa-3d56-4ad9-92dc-575836c143d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29e8a4220>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f093df0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a43d3df0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a392bee0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29e8a4100>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a8c23eb0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29e8a7ac0>]\n"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.utils.normalize import get_mans_ues_metrics\n",
    "\n",
    "# Load and concatenate all UE values and metrics for all test datasets\n",
    "ues, gen_metrics = get_mans_ues_metrics(EVAL_MAN_PATHS.values(), UE_METHOD_NAMES, GEN_METRIC_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a4b2ea-c66b-4feb-8d87-f08f9d50f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = get_confidences(fitted_normalizers, ues)\n",
    "mses = calculate_mses(confidences, gen_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcc5210-6255-481e-b41c-839cf989cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mses(ax, mses, title):\n",
    "    ax.bar(x-0.3, mses['min_max']['AlignScore'], width=0.2, color='g', align='center', label='MinMax')\n",
    "    ax.bar(x-0.1, mses['quantile']['AlignScore'], width=0.2, color='b', align='center', label='Quantile')\n",
    "    ax.bar(x+0.1, mses['binned_pcc']['AlignScore'], width=0.2, color='tab:olive', align='center', label='Binned')\n",
    "    ax.bar(x+0.3, mses['isotonic_pcc']['AlignScore'], width=0.2, color='r', align='center', label='Isotonic')\n",
    "\n",
    "    ax.set_xticks(range(len(UE_METHOD_NAMES)), UE_METHOD_NAMES, rotation=90)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a0e47d-c28e-43a4-9b32-6d8498fc91a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array(list(range(len(UE_METHOD_NAMES))))\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "\n",
    "plot_mses(ax, mses, 'MSE between true AlignScore and confidence')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "f.legend(handles, labels, bbox_to_anchor=(1.13, 0.97))\n",
    "\n",
    "plt.tight_layout()\n",
    "# Change this to plt.show() to display inline\n",
    "plt.savefig(f'normalization_mse_total.pdf')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ab75e-1c83-4fdc-a5db-186161d02f58",
   "metadata": {},
   "source": [
    "### OOD Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3cc6137-dfec-4754-8d0d-0305c0ebef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290a82dd0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2927dbee0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290aa0610>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290ab4760>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290ab41c0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290a82920>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2af5b8910>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x299d83a30>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x1486fbee0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x299da06a0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x299db46a0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x299db4160>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x299d82770>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a95abeb0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x295483a30>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x297e5bee0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29543b340>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x295439390>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x297e6fd90>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2954805b0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f13fdf0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f183880>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a1c5bf70>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f137dc0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f135a80>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a1c6be80>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29f180790>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a9527fa0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x296f83b50>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29a45bee0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x296f37d30>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x283c640a0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29a46be20>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x296f00ca0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2b06fc130>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290081690>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29335bf10>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290037ac0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290035720>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29336bdf0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x290000ca0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x103625c30>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x298c83a30>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29be5bee0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x28b5923e0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x298c35570>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29be6bdc0>]\n",
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x298c007f0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2b05fbf40>]\n"
     ]
    }
   ],
   "source": [
    "ood_confidences = {}\n",
    "ood_mses = {}\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    # Fit normalizers excluding current OOD dataset from train set\n",
    "    train_man_paths_wo_dataset = [value for key, value in TRAIN_MAN_PATHS.items() if key != dataset_name]\n",
    "    train_man_paths_wo_dataset = get_man_paths_list(train_man_paths_wo_dataset)\n",
    "    os.system(f'HYDRA_CONFIG={config_path} polygraph_normalize save_path=\"./ood_{dataset_name}\" man_paths={train_man_paths_wo_dataset}')\n",
    "\n",
    "    # Get UE and metric values for OOD dataset\n",
    "    ues, gen_metrics = get_mans_ues_metrics([EVAL_MAN_PATHS[dataset_name]], UE_METHOD_NAMES, GEN_METRIC_NAMES)\n",
    "\n",
    "    with open(f'./ood_{dataset_name}/fitted_normalizers.json', 'rb') as f:\n",
    "        fitted_normalizers = pickle.load(f)\n",
    "\n",
    "    ood_confidences[dataset_name] = get_confidences(fitted_normalizers, ues)\n",
    "    ood_mses[dataset_name] = calculate_mses(ood_confidences[dataset_name], gen_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7851db-8d4d-4d7a-985a-8f02697c3120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# If number of datasets is not even, we display an even part in pairs, and then last one separately\n",
    "is_even = (len(DATASET_NAMES) % 2 == 0)\n",
    "even_datasets_subset = DATASET_NAMES if is_even else DATASET_NAMES[:-1]\n",
    "dataset_pairs = [even_datasets_subset[i:i + 2] for i in range(0, len(even_datasets_subset), 2)]\n",
    "\n",
    "x = np.array(list(range(len(UE_METHOD_NAMES))))\n",
    "\n",
    "for dataset_pair in dataset_pairs:\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = GridSpec(1, 2, figure=fig)\n",
    "\n",
    "    for i, dataset_name in enumerate(dataset_pair):\n",
    "        ax = fig.add_subplot(gs[i//2,i%2])\n",
    "        plot_mses(ax, ood_mses[dataset_name], f'MSE between true AlignScore and confidence: {dataset_name}')\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    fig.legend(handles, labels, bbox_to_anchor=(1.1, 0.95))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Change this to plt.show() to display inline\n",
    "    plt.savefig(f'normalization_mse_ood_{dataset_name}.pdf')\n",
    "    plt.clf()\n",
    "\n",
    "if not is_even:\n",
    "    dataset_name = DATASET_NAMES[-1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0,0])\n",
    "\n",
    "    plot_mses(ax, ood_mses[dataset_name], f'MSE between true AlignScore and confidence: {dataset_name}')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, bbox_to_anchor=(1.2, 0.95))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Change this to plt.show() to display inline\n",
    "    plt.savefig(f'normalization_mse_ood_{dataset_name}.pdf')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5e4be-eec1-45da-b1ad-ec0662707c34",
   "metadata": {},
   "source": [
    "### PRR change relative to raw uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25439bd3-1e06-4770-ba89-3eab823e34ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29df27430>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.027419e-05</td>\n",
       "      <td>0.045624</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>6.973301e-05</td>\n",
       "      <td>0.017120</td>\n",
       "      <td>1.025691e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.562246e-05</td>\n",
       "      <td>0.071875</td>\n",
       "      <td>1.270313e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.521409e-05</td>\n",
       "      <td>0.023563</td>\n",
       "      <td>1.318007e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>6.973301e-05</td>\n",
       "      <td>0.017120</td>\n",
       "      <td>1.025691e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.897441e-05</td>\n",
       "      <td>-0.040076</td>\n",
       "      <td>-1.698985e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.133538e-04</td>\n",
       "      <td>-0.005526</td>\n",
       "      <td>9.455222e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.969507e-05</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>3.595616e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>7.251505e-06</td>\n",
       "      <td>0.026097</td>\n",
       "      <td>2.124271e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.378414e-05</td>\n",
       "      <td>0.240101</td>\n",
       "      <td>4.681824e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.778765e-04</td>\n",
       "      <td>-0.058829</td>\n",
       "      <td>-1.491817e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.041784e-05</td>\n",
       "      <td>0.215343</td>\n",
       "      <td>1.240209e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386922e-05</td>\n",
       "      <td>0.149697</td>\n",
       "      <td>5.526781e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.354954</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.588346e-04</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>-0.002508</td>\n",
       "      <td>-2.420026e-05</td>\n",
       "      <td>0.030214</td>\n",
       "      <td>-3.762161e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>0.003211</td>\n",
       "      <td>6.473588e-07</td>\n",
       "      <td>0.201822</td>\n",
       "      <td>2.246976e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.531779e-05</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>6.546212e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.493549e-05</td>\n",
       "      <td>0.219192</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>-0.000006</td>\n",
       "      <td>6.147003e-04</td>\n",
       "      <td>0.173752</td>\n",
       "      <td>-1.045500e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>0.002964</td>\n",
       "      <td>-1.077129e-03</td>\n",
       "      <td>0.044121</td>\n",
       "      <td>6.747251e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>0.000449</td>\n",
       "      <td>3.501979e-04</td>\n",
       "      <td>0.131586</td>\n",
       "      <td>-1.027675e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>0.007059</td>\n",
       "      <td>4.620739e-05</td>\n",
       "      <td>0.192506</td>\n",
       "      <td>1.019436e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>-0.000078</td>\n",
       "      <td>2.094294e-04</td>\n",
       "      <td>0.037503</td>\n",
       "      <td>9.463821e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MinMax      Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                 0.000000  3.027419e-05    0.045624   \n",
       "Perplexity                                 0.000054  6.973301e-05    0.017120   \n",
       "MeanTokenEntropy                           0.000000  8.562246e-05    0.071875   \n",
       "MeanPointwiseMutualInformation             0.000000 -1.521409e-05    0.023563   \n",
       "MeanConditionalPointwiseMutualInformation  0.000054  6.973301e-05    0.017120   \n",
       "PTrue                                      0.000000  5.897441e-05   -0.040076   \n",
       "PTrueSampling                              0.000000  4.133538e-04   -0.005526   \n",
       "MonteCarloSequenceEntropy                  0.000000  6.969507e-05    0.072111   \n",
       "MonteCarloNormalizedSequenceEntropy        0.000043  7.251505e-06    0.026097   \n",
       "LexicalSimilarity_rouge1                   0.000000  6.378414e-05    0.240101   \n",
       "LexicalSimilarity_rouge2                   0.000000 -8.778765e-04   -0.058829   \n",
       "LexicalSimilarity_rougeL                   0.000000  2.041784e-05    0.215343   \n",
       "LexicalSimilarity_BLEU                     0.000000  1.386922e-05    0.149697   \n",
       "NumSemSets                                 0.000000  0.000000e+00    0.354954   \n",
       "EigValLaplacian_NLI_score_entail           0.000000  8.588346e-04    0.027926   \n",
       "EigValLaplacian_NLI_score_contra          -0.002508 -2.420026e-05    0.030214   \n",
       "EigValLaplacian_Jaccard_score              0.003211  6.473588e-07    0.201822   \n",
       "DegMat_NLI_score_entail                    0.000000  3.531779e-05    0.035874   \n",
       "DegMat_NLI_score_contra                    0.000000  3.493549e-05    0.219192   \n",
       "DegMat_Jaccard_score                      -0.000006  6.147003e-04    0.173752   \n",
       "Eccentricity_NLI_score_entail              0.002964 -1.077129e-03    0.044121   \n",
       "Eccentricity_NLI_score_contra              0.000449  3.501979e-04    0.131586   \n",
       "Eccentricity_Jaccard_score                 0.007059  4.620739e-05    0.192506   \n",
       "SemanticEntropy                           -0.000078  2.094294e-04    0.037503   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                 0.000000e+00  \n",
       "Perplexity                                 1.025691e-04  \n",
       "MeanTokenEntropy                           1.270313e-04  \n",
       "MeanPointwiseMutualInformation             1.318007e-06  \n",
       "MeanConditionalPointwiseMutualInformation  1.025691e-04  \n",
       "PTrue                                     -1.698985e-02  \n",
       "PTrueSampling                              9.455222e-02  \n",
       "MonteCarloSequenceEntropy                  3.595616e-04  \n",
       "MonteCarloNormalizedSequenceEntropy        2.124271e-04  \n",
       "LexicalSimilarity_rouge1                   4.681824e-04  \n",
       "LexicalSimilarity_rouge2                  -1.491817e-02  \n",
       "LexicalSimilarity_rougeL                   1.240209e-03  \n",
       "LexicalSimilarity_BLEU                     5.526781e-07  \n",
       "NumSemSets                                 0.000000e+00  \n",
       "EigValLaplacian_NLI_score_entail           0.000000e+00  \n",
       "EigValLaplacian_NLI_score_contra          -3.762161e-03  \n",
       "EigValLaplacian_Jaccard_score              2.246976e-03  \n",
       "DegMat_NLI_score_entail                    6.546212e-04  \n",
       "DegMat_NLI_score_contra                    0.000000e+00  \n",
       "DegMat_Jaccard_score                      -1.045500e-05  \n",
       "Eccentricity_NLI_score_entail              6.747251e-04  \n",
       "Eccentricity_NLI_score_contra             -1.027675e-03  \n",
       "Eccentricity_Jaccard_score                 1.019436e-02  \n",
       "SemanticEntropy                            9.463821e-05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a0ec02b0>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.014490</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.077725</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.051275</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.372224</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.077725</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.059528</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.100969</td>\n",
       "      <td>5.841522e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.026921</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.023329</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>-3.780490e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.026084</td>\n",
       "      <td>7.933510e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.141791</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>5.516324e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.002955</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.117703</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0.041052</td>\n",
       "      <td>-7.082841e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>6.594658e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.066093</td>\n",
       "      <td>-1.538722e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MinMax  Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                 0.000000  0.000022   -0.014490   \n",
       "Perplexity                                 0.000000  0.000035    0.077725   \n",
       "MeanTokenEntropy                           0.000000  0.000088    0.051275   \n",
       "MeanPointwiseMutualInformation             0.000000  0.000218    0.372224   \n",
       "MeanConditionalPointwiseMutualInformation  0.000000  0.000035    0.077725   \n",
       "PTrue                                      0.000000 -0.000097    0.014158   \n",
       "PTrueSampling                              0.000000  0.001666    0.059528   \n",
       "MonteCarloSequenceEntropy                  0.000199  0.000205    0.100969   \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000 -0.000130    0.026921   \n",
       "LexicalSimilarity_rouge1                   0.000000  0.000050    0.017079   \n",
       "LexicalSimilarity_rouge2                   0.000000  0.000051    0.023329   \n",
       "LexicalSimilarity_rougeL                   0.000000  0.000074    0.003260   \n",
       "LexicalSimilarity_BLEU                     0.000000  0.000084    0.026084   \n",
       "NumSemSets                                 0.000000  0.000000    0.063453   \n",
       "EigValLaplacian_NLI_score_entail           0.000000 -0.000017    0.001113   \n",
       "EigValLaplacian_NLI_score_contra           0.000000  0.000050    0.141791   \n",
       "EigValLaplacian_Jaccard_score              0.000000 -0.000057    0.013408   \n",
       "DegMat_NLI_score_entail                    0.000000  0.000011   -0.002955   \n",
       "DegMat_NLI_score_contra                    0.000000  0.000011    0.117703   \n",
       "DegMat_Jaccard_score                       0.000000 -0.000087    0.041052   \n",
       "Eccentricity_NLI_score_entail              0.000000  0.000023    0.007306   \n",
       "Eccentricity_NLI_score_contra              0.000000 -0.000129    0.000619   \n",
       "Eccentricity_Jaccard_score                 0.000000  0.000024    0.013989   \n",
       "SemanticEntropy                            0.000098  0.000135    0.066093   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                 0.000000e+00  \n",
       "Perplexity                                 0.000000e+00  \n",
       "MeanTokenEntropy                           0.000000e+00  \n",
       "MeanPointwiseMutualInformation             0.000000e+00  \n",
       "MeanConditionalPointwiseMutualInformation  0.000000e+00  \n",
       "PTrue                                      0.000000e+00  \n",
       "PTrueSampling                              0.000000e+00  \n",
       "MonteCarloSequenceEntropy                  5.841522e-03  \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000e+00  \n",
       "LexicalSimilarity_rouge1                   0.000000e+00  \n",
       "LexicalSimilarity_rouge2                   0.000000e+00  \n",
       "LexicalSimilarity_rougeL                  -3.780490e-06  \n",
       "LexicalSimilarity_BLEU                     7.933510e-07  \n",
       "NumSemSets                                 0.000000e+00  \n",
       "EigValLaplacian_NLI_score_entail           0.000000e+00  \n",
       "EigValLaplacian_NLI_score_contra           0.000000e+00  \n",
       "EigValLaplacian_Jaccard_score              5.516324e-07  \n",
       "DegMat_NLI_score_entail                    0.000000e+00  \n",
       "DegMat_NLI_score_contra                    0.000000e+00  \n",
       "DegMat_Jaccard_score                      -7.082841e-05  \n",
       "Eccentricity_NLI_score_entail              6.594658e-05  \n",
       "Eccentricity_NLI_score_contra              0.000000e+00  \n",
       "Eccentricity_Jaccard_score                 0.000000e+00  \n",
       "SemanticEntropy                           -1.538722e-04  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29df24dc0>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.871689e-05</td>\n",
       "      <td>0.034351</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.757970e-06</td>\n",
       "      <td>0.076598</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.218778e-05</td>\n",
       "      <td>-0.078761</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.981897e-05</td>\n",
       "      <td>0.171447</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.757970e-06</td>\n",
       "      <td>0.076598</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>-0.010234</td>\n",
       "      <td>-1.012989e-02</td>\n",
       "      <td>-0.755300</td>\n",
       "      <td>-5.903252e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>-0.002085</td>\n",
       "      <td>-1.342966e-03</td>\n",
       "      <td>-0.099219</td>\n",
       "      <td>-2.084942e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287856e-05</td>\n",
       "      <td>0.018792</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.304778e-07</td>\n",
       "      <td>0.071458</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.144289e-03</td>\n",
       "      <td>0.165480</td>\n",
       "      <td>-3.125111e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.366665</td>\n",
       "      <td>-1.632434e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.233570e-03</td>\n",
       "      <td>0.107575</td>\n",
       "      <td>6.708808e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.360847e-04</td>\n",
       "      <td>0.041270</td>\n",
       "      <td>-1.705705e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.059265</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.667384e-06</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.770779e-04</td>\n",
       "      <td>0.371505</td>\n",
       "      <td>-6.087704e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>-0.001051</td>\n",
       "      <td>2.969064e-04</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>1.425219e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.309233e-06</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.316850e-05</td>\n",
       "      <td>0.182786</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.572243e-04</td>\n",
       "      <td>0.062193</td>\n",
       "      <td>-6.879691e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-8.696721e-04</td>\n",
       "      <td>0.044149</td>\n",
       "      <td>-8.114015e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>0.001035</td>\n",
       "      <td>-5.351811e-04</td>\n",
       "      <td>0.150937</td>\n",
       "      <td>1.107026e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>0.003407</td>\n",
       "      <td>4.681425e-04</td>\n",
       "      <td>0.049646</td>\n",
       "      <td>3.974744e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.950968e-05</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MinMax      Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                 0.000000  2.871689e-05    0.034351   \n",
       "Perplexity                                 0.000000 -7.757970e-06    0.076598   \n",
       "MeanTokenEntropy                           0.000000  4.218778e-05   -0.078761   \n",
       "MeanPointwiseMutualInformation             0.000000  2.981897e-05    0.171447   \n",
       "MeanConditionalPointwiseMutualInformation  0.000000 -7.757970e-06    0.076598   \n",
       "PTrue                                     -0.010234 -1.012989e-02   -0.755300   \n",
       "PTrueSampling                             -0.002085 -1.342966e-03   -0.099219   \n",
       "MonteCarloSequenceEntropy                  0.000000  1.287856e-05    0.018792   \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000  6.304778e-07    0.071458   \n",
       "LexicalSimilarity_rouge1                   0.000000 -1.144289e-03    0.165480   \n",
       "LexicalSimilarity_rouge2                   0.000000  0.000000e+00   -0.366665   \n",
       "LexicalSimilarity_rougeL                   0.000000  1.233570e-03    0.107575   \n",
       "LexicalSimilarity_BLEU                     0.000000 -1.360847e-04    0.041270   \n",
       "NumSemSets                                 0.000000  0.000000e+00    0.059265   \n",
       "EigValLaplacian_NLI_score_entail           0.000000  5.667384e-06    0.020830   \n",
       "EigValLaplacian_NLI_score_contra           0.000000  2.770779e-04    0.371505   \n",
       "EigValLaplacian_Jaccard_score             -0.001051  2.969064e-04    0.097403   \n",
       "DegMat_NLI_score_entail                    0.000000  2.309233e-06    0.020859   \n",
       "DegMat_NLI_score_contra                    0.000000  4.316850e-05    0.182786   \n",
       "DegMat_Jaccard_score                      -0.000002 -1.572243e-04    0.062193   \n",
       "Eccentricity_NLI_score_entail             -0.001316 -8.696721e-04    0.044149   \n",
       "Eccentricity_NLI_score_contra              0.001035 -5.351811e-04    0.150937   \n",
       "Eccentricity_Jaccard_score                 0.003407  4.681425e-04    0.049646   \n",
       "SemanticEntropy                            0.000000  1.950968e-05    0.037600   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                 0.000000e+00  \n",
       "Perplexity                                 0.000000e+00  \n",
       "MeanTokenEntropy                           0.000000e+00  \n",
       "MeanPointwiseMutualInformation             0.000000e+00  \n",
       "MeanConditionalPointwiseMutualInformation  0.000000e+00  \n",
       "PTrue                                     -5.903252e-01  \n",
       "PTrueSampling                             -2.084942e-03  \n",
       "MonteCarloSequenceEntropy                  0.000000e+00  \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000e+00  \n",
       "LexicalSimilarity_rouge1                  -3.125111e-04  \n",
       "LexicalSimilarity_rouge2                  -1.632434e-01  \n",
       "LexicalSimilarity_rougeL                   6.708808e-05  \n",
       "LexicalSimilarity_BLEU                    -1.705705e-04  \n",
       "NumSemSets                                 0.000000e+00  \n",
       "EigValLaplacian_NLI_score_entail           0.000000e+00  \n",
       "EigValLaplacian_NLI_score_contra          -6.087704e-07  \n",
       "EigValLaplacian_Jaccard_score              1.425219e-03  \n",
       "DegMat_NLI_score_entail                    0.000000e+00  \n",
       "DegMat_NLI_score_contra                    0.000000e+00  \n",
       "DegMat_Jaccard_score                      -6.879691e-05  \n",
       "Eccentricity_NLI_score_entail             -8.114015e-04  \n",
       "Eccentricity_NLI_score_contra              1.107026e-03  \n",
       "Eccentricity_Jaccard_score                 3.974744e-03  \n",
       "SemanticEntropy                            0.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a9e91d20>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>-1.436972e-05</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.076625</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.210053</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.102983</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.449750</td>\n",
       "      <td>0.086119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.003844</td>\n",
       "      <td>-0.006621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>-1.436972e-05</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.068503</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.040447</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.340906</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>-0.103201</td>\n",
       "      <td>-0.065013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.264497</td>\n",
       "      <td>-0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.231389</td>\n",
       "      <td>0.000618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545821</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>2.045660e-03</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.003218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>4.496666e-04</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.111092</td>\n",
       "      <td>0.001348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>2.197808e-04</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.105752</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>5.018947e-07</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.021656</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>4.503513e-04</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.186992</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>7.792802e-05</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.207556</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>5.533344e-03</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.040825</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>-1.922317e-03</td>\n",
       "      <td>-0.000577</td>\n",
       "      <td>0.190043</td>\n",
       "      <td>-0.001969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>-1.752294e-03</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.372418</td>\n",
       "      <td>0.002787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>-1.436972e-05</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>0.055701</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 MinMax  Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                -1.436972e-05  0.000125    0.076625   \n",
       "Perplexity                                 0.000000e+00 -0.000021    0.032847   \n",
       "MeanTokenEntropy                           0.000000e+00 -0.000025    0.210053   \n",
       "MeanPointwiseMutualInformation             0.000000e+00  0.000046   -0.102983   \n",
       "MeanConditionalPointwiseMutualInformation  0.000000e+00 -0.000021    0.032847   \n",
       "PTrue                                      0.000000e+00 -0.000144    0.449750   \n",
       "PTrueSampling                              0.000000e+00 -0.000199   -0.003844   \n",
       "MonteCarloSequenceEntropy                 -1.436972e-05 -0.000104    0.068503   \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000e+00 -0.000067    0.040447   \n",
       "LexicalSimilarity_rouge1                   0.000000e+00  0.000069    0.340906   \n",
       "LexicalSimilarity_rouge2                   0.000000e+00  0.000080   -0.103201   \n",
       "LexicalSimilarity_rougeL                   0.000000e+00  0.000052    0.264497   \n",
       "LexicalSimilarity_BLEU                     0.000000e+00  0.000096    0.231389   \n",
       "NumSemSets                                 0.000000e+00  0.000000    0.545821   \n",
       "EigValLaplacian_NLI_score_entail           2.045660e-03  0.000421    0.022331   \n",
       "EigValLaplacian_NLI_score_contra           4.496666e-04 -0.000818    0.111092   \n",
       "EigValLaplacian_Jaccard_score              2.197808e-04  0.000011    0.105752   \n",
       "DegMat_NLI_score_entail                    5.018947e-07  0.000070    0.021656   \n",
       "DegMat_NLI_score_contra                    4.503513e-04  0.000342    0.186992   \n",
       "DegMat_Jaccard_score                       7.792802e-05  0.000020    0.207556   \n",
       "Eccentricity_NLI_score_entail              5.533344e-03  0.001453    0.040825   \n",
       "Eccentricity_NLI_score_contra             -1.922317e-03 -0.000577    0.190043   \n",
       "Eccentricity_Jaccard_score                -1.752294e-03 -0.000077    0.372418   \n",
       "SemanticEntropy                           -1.436972e-05 -0.000135    0.055701   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                    -0.000014  \n",
       "Perplexity                                     0.000000  \n",
       "MeanTokenEntropy                               0.000000  \n",
       "MeanPointwiseMutualInformation                 0.000000  \n",
       "MeanConditionalPointwiseMutualInformation      0.000000  \n",
       "PTrue                                          0.086119  \n",
       "PTrueSampling                                 -0.006621  \n",
       "MonteCarloSequenceEntropy                     -0.000014  \n",
       "MonteCarloNormalizedSequenceEntropy           -0.000014  \n",
       "LexicalSimilarity_rouge1                       0.000057  \n",
       "LexicalSimilarity_rouge2                      -0.065013  \n",
       "LexicalSimilarity_rougeL                      -0.000167  \n",
       "LexicalSimilarity_BLEU                         0.000618  \n",
       "NumSemSets                                     0.000058  \n",
       "EigValLaplacian_NLI_score_entail               0.003218  \n",
       "EigValLaplacian_NLI_score_contra               0.001348  \n",
       "EigValLaplacian_Jaccard_score                  0.000089  \n",
       "DegMat_NLI_score_entail                        0.000003  \n",
       "DegMat_NLI_score_contra                        0.000450  \n",
       "DegMat_Jaccard_score                           0.008547  \n",
       "Eccentricity_NLI_score_entail                  0.005657  \n",
       "Eccentricity_NLI_score_contra                 -0.001969  \n",
       "Eccentricity_Jaccard_score                     0.002787  \n",
       "SemanticEntropy                               -0.000014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a74d3460>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.146019</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.023779</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.040141</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.015287</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.023779</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.124073</td>\n",
       "      <td>3.356028e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>0.135484</td>\n",
       "      <td>1.691108e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.202491</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000291</td>\n",
       "      <td>-0.028530</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>9.814974e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>-0.007962</td>\n",
       "      <td>-1.674886e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>0.047123</td>\n",
       "      <td>-2.278422e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.014189</td>\n",
       "      <td>3.864723e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133410</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.068790</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.009853</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>-8.503347e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.003990</td>\n",
       "      <td>0.110534</td>\n",
       "      <td>-6.604477e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>-0.029112</td>\n",
       "      <td>-5.617697e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>0.058647</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.006096</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>0.027298</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.015822</td>\n",
       "      <td>-4.829143e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.082663</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MinMax  Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                 0.000000 -0.000010    0.146019   \n",
       "Perplexity                                 0.000000 -0.000024    0.023779   \n",
       "MeanTokenEntropy                           0.000000  0.000081    0.040141   \n",
       "MeanPointwiseMutualInformation             0.000000  0.000014    0.015287   \n",
       "MeanConditionalPointwiseMutualInformation  0.000000 -0.000024    0.023779   \n",
       "PTrue                                      0.000000 -0.000014    0.124073   \n",
       "PTrueSampling                              0.000000 -0.000205    0.135484   \n",
       "MonteCarloSequenceEntropy                  0.000000 -0.000028    0.202491   \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000 -0.000291   -0.028530   \n",
       "LexicalSimilarity_rouge1                   0.000000 -0.000058    0.054083   \n",
       "LexicalSimilarity_rouge2                   0.000000 -0.000454   -0.007962   \n",
       "LexicalSimilarity_rougeL                   0.000000 -0.000173    0.047123   \n",
       "LexicalSimilarity_BLEU                     0.000000 -0.000108    0.014189   \n",
       "NumSemSets                                 0.000000  0.000000    0.133410   \n",
       "EigValLaplacian_NLI_score_entail           0.000000 -0.000169    0.068790   \n",
       "EigValLaplacian_NLI_score_contra           0.000000  0.000067    0.009853   \n",
       "EigValLaplacian_Jaccard_score              0.000000 -0.000116    0.019848   \n",
       "DegMat_NLI_score_entail                   -0.000007 -0.003990    0.110534   \n",
       "DegMat_NLI_score_contra                    0.000000  0.003064   -0.029112   \n",
       "DegMat_Jaccard_score                       0.000000 -0.000371    0.058647   \n",
       "Eccentricity_NLI_score_entail              0.000000  0.000036   -0.006096   \n",
       "Eccentricity_NLI_score_contra              0.000000 -0.000517    0.027298   \n",
       "Eccentricity_Jaccard_score                -0.000006  0.000327    0.015822   \n",
       "SemanticEntropy                            0.000000 -0.000019    0.082663   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                 0.000000e+00  \n",
       "Perplexity                                 0.000000e+00  \n",
       "MeanTokenEntropy                           0.000000e+00  \n",
       "MeanPointwiseMutualInformation             0.000000e+00  \n",
       "MeanConditionalPointwiseMutualInformation  0.000000e+00  \n",
       "PTrue                                      3.356028e-02  \n",
       "PTrueSampling                              1.691108e-02  \n",
       "MonteCarloSequenceEntropy                  0.000000e+00  \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000e+00  \n",
       "LexicalSimilarity_rouge1                   9.814974e-04  \n",
       "LexicalSimilarity_rouge2                  -1.674886e-06  \n",
       "LexicalSimilarity_rougeL                  -2.278422e-05  \n",
       "LexicalSimilarity_BLEU                     3.864723e-08  \n",
       "NumSemSets                                 0.000000e+00  \n",
       "EigValLaplacian_NLI_score_entail           0.000000e+00  \n",
       "EigValLaplacian_NLI_score_contra           0.000000e+00  \n",
       "EigValLaplacian_Jaccard_score             -8.503347e-07  \n",
       "DegMat_NLI_score_entail                   -6.604477e-06  \n",
       "DegMat_NLI_score_contra                   -5.617697e-03  \n",
       "DegMat_Jaccard_score                       0.000000e+00  \n",
       "Eccentricity_NLI_score_entail              0.000000e+00  \n",
       "Eccentricity_NLI_score_contra              0.000000e+00  \n",
       "Eccentricity_Jaccard_score                -4.829143e-06  \n",
       "SemanticEntropy                            0.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a8b7bfd0>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.093635</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>0.016244</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>-0.067971</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.270316</td>\n",
       "      <td>4.404489e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.054360</td>\n",
       "      <td>2.749514e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.233209</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.042520</td>\n",
       "      <td>3.040329e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>2.522466e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.034766</td>\n",
       "      <td>1.841592e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.034885</td>\n",
       "      <td>1.873914e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.036841</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.064042</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.029495</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001646</td>\n",
       "      <td>0.197130</td>\n",
       "      <td>-1.197602e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.057659</td>\n",
       "      <td>5.471336e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>1.068834e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.035399</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.054728</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.050536</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.164023</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MinMax  Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                 0.000000 -0.000154    0.093635   \n",
       "Perplexity                                 0.000000  0.000233    0.004554   \n",
       "MeanTokenEntropy                           0.000000 -0.000198    0.016244   \n",
       "MeanPointwiseMutualInformation             0.000000  0.000140   -0.067971   \n",
       "MeanConditionalPointwiseMutualInformation  0.000000  0.000233    0.004554   \n",
       "PTrue                                      0.000000 -0.000017    0.270316   \n",
       "PTrueSampling                              0.000000 -0.000116    0.054360   \n",
       "MonteCarloSequenceEntropy                  0.000000  0.000113    0.233209   \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000  0.000165   -0.015257   \n",
       "LexicalSimilarity_rouge1                   0.000000 -0.000724    0.042520   \n",
       "LexicalSimilarity_rouge2                   0.000000 -0.000110    0.003769   \n",
       "LexicalSimilarity_rougeL                   0.000000  0.000690    0.034766   \n",
       "LexicalSimilarity_BLEU                     0.000000  0.000010    0.034885   \n",
       "NumSemSets                                 0.000000  0.000000    0.284211   \n",
       "EigValLaplacian_NLI_score_entail           0.000000 -0.000441    0.036841   \n",
       "EigValLaplacian_NLI_score_contra           0.000000  0.000722    0.064042   \n",
       "EigValLaplacian_Jaccard_score              0.000000 -0.000132    0.029495   \n",
       "DegMat_NLI_score_entail                    0.000000 -0.001646    0.197130   \n",
       "DegMat_NLI_score_contra                   -0.000008  0.000042    0.057659   \n",
       "DegMat_Jaccard_score                       0.000000  0.000085    0.013775   \n",
       "Eccentricity_NLI_score_entail              0.000000  0.000384    0.035399   \n",
       "Eccentricity_NLI_score_contra              0.000000 -0.000060    0.054728   \n",
       "Eccentricity_Jaccard_score                 0.000000  0.000400    0.050536   \n",
       "SemanticEntropy                            0.000000 -0.000131    0.164023   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                 0.000000e+00  \n",
       "Perplexity                                 0.000000e+00  \n",
       "MeanTokenEntropy                           0.000000e+00  \n",
       "MeanPointwiseMutualInformation             0.000000e+00  \n",
       "MeanConditionalPointwiseMutualInformation  0.000000e+00  \n",
       "PTrue                                      4.404489e-04  \n",
       "PTrueSampling                              2.749514e-04  \n",
       "MonteCarloSequenceEntropy                  0.000000e+00  \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000e+00  \n",
       "LexicalSimilarity_rouge1                   3.040329e-05  \n",
       "LexicalSimilarity_rouge2                   2.522466e-08  \n",
       "LexicalSimilarity_rougeL                   1.841592e-05  \n",
       "LexicalSimilarity_BLEU                     1.873914e-05  \n",
       "NumSemSets                                 0.000000e+00  \n",
       "EigValLaplacian_NLI_score_entail           0.000000e+00  \n",
       "EigValLaplacian_NLI_score_contra           0.000000e+00  \n",
       "EigValLaplacian_Jaccard_score              0.000000e+00  \n",
       "DegMat_NLI_score_entail                   -1.197602e-04  \n",
       "DegMat_NLI_score_contra                    5.471336e-05  \n",
       "DegMat_Jaccard_score                       1.068834e-05  \n",
       "Eccentricity_NLI_score_entail              0.000000e+00  \n",
       "Eccentricity_NLI_score_contra              0.000000e+00  \n",
       "Eccentricity_Jaccard_score                 0.000000e+00  \n",
       "SemanticEntropy                            0.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x29df9fc40>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>-0.021840</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.011166</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.045119</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.011166</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.050726</td>\n",
       "      <td>3.519649e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.017532</td>\n",
       "      <td>1.190431e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>-0.014168</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000438</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.083852</td>\n",
       "      <td>-5.900041e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.023779</td>\n",
       "      <td>7.215371e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.060013</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>3.176101e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069615</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.002182</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>0.118055</td>\n",
       "      <td>2.249237e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.015665</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.031645</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.063957</td>\n",
       "      <td>1.794258e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.044428</td>\n",
       "      <td>2.952209e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.035489</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001875</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           MinMax  Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                    0.0  0.000124   -0.021840   \n",
       "Perplexity                                    0.0 -0.000001    0.011166   \n",
       "MeanTokenEntropy                              0.0  0.000081    0.019600   \n",
       "MeanPointwiseMutualInformation                0.0  0.000111    0.045119   \n",
       "MeanConditionalPointwiseMutualInformation     0.0 -0.000001    0.011166   \n",
       "PTrue                                         0.0 -0.000099   -0.050726   \n",
       "PTrueSampling                                 0.0  0.000013   -0.017532   \n",
       "MonteCarloSequenceEntropy                     0.0  0.000055   -0.014168   \n",
       "MonteCarloNormalizedSequenceEntropy           0.0 -0.000012   -0.000438   \n",
       "LexicalSimilarity_rouge1                      0.0  0.000004    0.083852   \n",
       "LexicalSimilarity_rouge2                      0.0 -0.000058    0.023779   \n",
       "LexicalSimilarity_rougeL                      0.0  0.000044    0.060013   \n",
       "LexicalSimilarity_BLEU                        0.0 -0.000016    0.004661   \n",
       "NumSemSets                                    0.0  0.000000    0.069615   \n",
       "EigValLaplacian_NLI_score_entail              0.0 -0.000157   -0.000216   \n",
       "EigValLaplacian_NLI_score_contra              0.0  0.000026   -0.002182   \n",
       "EigValLaplacian_Jaccard_score                 0.0 -0.000103    0.118055   \n",
       "DegMat_NLI_score_entail                       0.0  0.000040   -0.015665   \n",
       "DegMat_NLI_score_contra                       0.0 -0.000065    0.031645   \n",
       "DegMat_Jaccard_score                          0.0 -0.000005    0.063957   \n",
       "Eccentricity_NLI_score_entail                 0.0  0.000023    0.044428   \n",
       "Eccentricity_NLI_score_contra                 0.0  0.000069   -0.035489   \n",
       "Eccentricity_Jaccard_score                    0.0 -0.001875    0.005577   \n",
       "SemanticEntropy                               0.0  0.000056    0.001172   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                 0.000000e+00  \n",
       "Perplexity                                 0.000000e+00  \n",
       "MeanTokenEntropy                           0.000000e+00  \n",
       "MeanPointwiseMutualInformation             0.000000e+00  \n",
       "MeanConditionalPointwiseMutualInformation  0.000000e+00  \n",
       "PTrue                                      3.519649e-04  \n",
       "PTrueSampling                              1.190431e-03  \n",
       "MonteCarloSequenceEntropy                  0.000000e+00  \n",
       "MonteCarloNormalizedSequenceEntropy        0.000000e+00  \n",
       "LexicalSimilarity_rouge1                  -5.900041e-05  \n",
       "LexicalSimilarity_rouge2                   7.215371e-04  \n",
       "LexicalSimilarity_rougeL                   0.000000e+00  \n",
       "LexicalSimilarity_BLEU                     3.176101e-05  \n",
       "NumSemSets                                 0.000000e+00  \n",
       "EigValLaplacian_NLI_score_entail           0.000000e+00  \n",
       "EigValLaplacian_NLI_score_contra           0.000000e+00  \n",
       "EigValLaplacian_Jaccard_score              2.249237e-06  \n",
       "DegMat_NLI_score_entail                    0.000000e+00  \n",
       "DegMat_NLI_score_contra                    0.000000e+00  \n",
       "DegMat_Jaccard_score                       1.794258e-08  \n",
       "Eccentricity_NLI_score_entail              2.952209e-03  \n",
       "Eccentricity_NLI_score_contra              0.000000e+00  \n",
       "Eccentricity_Jaccard_score                 0.000000e+00  \n",
       "SemanticEntropy                            0.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lm_polygraph.utils.normalize import filter_nans\n",
    "from lm_polygraph.ue_metrics.pred_rej_area import PredictionRejectionArea\n",
    "from lm_polygraph.ue_metrics.ue_metric import (\n",
    "    get_random_scores,\n",
    "    normalize_metric,\n",
    ")\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "ue_metric = PredictionRejectionArea()\n",
    "\n",
    "cols = ['MinMax', 'Quantile', 'Binned PCC', 'Isotonic PCC']\n",
    "\n",
    "# For each of the dataset we take all confidences calculated in OOD setting\n",
    "# and compare PRR of this to raw unnormalized UE\n",
    "for dataset_name, path in EVAL_MAN_PATHS.items():\n",
    "    res = {}\n",
    "    all_ues, all_gen_metrics = get_mans_ues_metrics([path], UE_METHOD_NAMES, GEN_METRIC_NAMES)\n",
    "    \n",
    "    for metric_name in GEN_METRIC_NAMES:\n",
    "        gen_metrics = all_gen_metrics[metric_name]\n",
    "        for ue_method_name in UE_METHOD_NAMES:\n",
    "            ues = all_ues[ue_method_name]\n",
    "            \n",
    "            filtered_metric, filtered_ues = filter_nans(gen_metrics, ues)\n",
    "\n",
    "            # -np.array() because we need to use UE, not confidence to calculate PRR\n",
    "            minmax_ues = -np.array(ood_confidences[dataset_name]['min_max'][metric_name][ue_method_name])\n",
    "            quantile_ues = -np.array(ood_confidences[dataset_name]['quantile'][metric_name][ue_method_name])\n",
    "            binned_pcc_ues = -np.array(ood_confidences[dataset_name]['binned_pcc'][metric_name][ue_method_name])\n",
    "            isotonic_pcc_ues = -np.array(ood_confidences[dataset_name]['isotonic_pcc'][metric_name][ue_method_name])\n",
    "\n",
    "            assert(len(filtered_ues) == len(minmax_ues))\n",
    "            \n",
    "            oracle_score = ue_metric(-filtered_metric, filtered_metric)\n",
    "            random_score = get_random_scores(ue_metric, filtered_metric)\n",
    "\n",
    "            raw_ue_metric_val = ue_metric(filtered_ues, filtered_metric)\n",
    "            raw_score = normalize_metric(raw_ue_metric_val, oracle_score, random_score)\n",
    "\n",
    "            minmax_ue_metric_val = ue_metric(minmax_ues, filtered_metric)\n",
    "            minmax_score = normalize_metric(minmax_ue_metric_val, oracle_score, random_score)\n",
    "            minmax_diff = raw_score - minmax_score\n",
    "\n",
    "            quantile_ue_metric_val = ue_metric(quantile_ues, filtered_metric)\n",
    "            quantile_score = normalize_metric(quantile_ue_metric_val, oracle_score, random_score)\n",
    "            quantile_diff = raw_score - quantile_score\n",
    "            \n",
    "            binned_pcc_ue_metric_val = ue_metric(binned_pcc_ues, filtered_metric)\n",
    "            binned_pcc_score = normalize_metric(binned_pcc_ue_metric_val, oracle_score, random_score)\n",
    "            binned_pcc_diff = raw_score - binned_pcc_score\n",
    "\n",
    "            isotonic_pcc_ue_metric_val = ue_metric(isotonic_pcc_ues, filtered_metric)\n",
    "            isotonic_pcc_score = normalize_metric(isotonic_pcc_ue_metric_val, oracle_score, random_score)\n",
    "            isotonic_pcc_diff = raw_score - isotonic_pcc_score\n",
    "\n",
    "            res[ue_method_name] = [minmax_diff, quantile_diff, binned_pcc_diff, isotonic_pcc_diff]\n",
    "\n",
    "    # Show table for each datasets that contains difference between raw UE PRR and PRR based on normalized confidence\n",
    "    # Lower is better, negative is best (means normalized confidence improves upon raw PRR\n",
    "    df = pd.DataFrame.from_dict(res, orient='index', columns=cols)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f04893f-0516-4855-a753-d4b8f5756539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a202f100>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a8b79bd0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a9e62140>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a9e454b0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a95053f0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2a9e84af0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x2af5bb9a0>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMax</th>\n",
       "      <th>Quantile</th>\n",
       "      <th>Binned PCC</th>\n",
       "      <th>Isotonic PCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaximumSequenceProbability</th>\n",
       "      <td>-5.721821e-06</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.050397</td>\n",
       "      <td>-5.721821e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>-8.007145e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>-7.831774e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanTokenEntropy</th>\n",
       "      <td>3.145503e-10</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>3.145503e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanPointwiseMutualInformation</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.313686</td>\n",
       "      <td>2.883105e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanConditionalPointwiseMutualInformation</th>\n",
       "      <td>-8.007145e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>-7.831774e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrue</th>\n",
       "      <td>-3.504455e-05</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.759635</td>\n",
       "      <td>-3.782925e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTrueSampling</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.375160</td>\n",
       "      <td>-5.205038e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloSequenceEntropy</th>\n",
       "      <td>-5.044093e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.068054</td>\n",
       "      <td>-5.044093e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonteCarloNormalizedSequenceEntropy</th>\n",
       "      <td>3.145507e-10</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>3.145507e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>-1.029108e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rouge2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.080649</td>\n",
       "      <td>-6.666348e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_rougeL</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000696</td>\n",
       "      <td>0.072246</td>\n",
       "      <td>-8.362908e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexicalSimilarity_BLEU</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.059771</td>\n",
       "      <td>-3.438847e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSemSets</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472821</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_entail</th>\n",
       "      <td>7.814226e-06</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>-2.335574e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_NLI_score_contra</th>\n",
       "      <td>-2.874865e-04</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>-1.899861e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EigValLaplacian_Jaccard_score</th>\n",
       "      <td>-2.516254e-05</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>4.344693e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_entail</th>\n",
       "      <td>2.338089e-08</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>-7.053086e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_NLI_score_contra</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.012561</td>\n",
       "      <td>-1.891540e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DegMat_Jaccard_score</th>\n",
       "      <td>-2.526807e-05</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.068838</td>\n",
       "      <td>-3.689794e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_entail</th>\n",
       "      <td>-1.484132e-04</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>1.973985e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_NLI_score_contra</th>\n",
       "      <td>-3.898841e-04</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>7.307482e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_Jaccard_score</th>\n",
       "      <td>8.248144e-04</td>\n",
       "      <td>-0.001261</td>\n",
       "      <td>0.095425</td>\n",
       "      <td>-1.006939e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemanticEntropy</th>\n",
       "      <td>-5.721821e-06</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.023679</td>\n",
       "      <td>-5.724935e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 MinMax  Quantile  Binned PCC  \\\n",
       "MaximumSequenceProbability                -5.721821e-06 -0.000007   -0.050397   \n",
       "Perplexity                                -8.007145e-06  0.000008    0.018182   \n",
       "MeanTokenEntropy                           3.145503e-10  0.000002   -0.008805   \n",
       "MeanPointwiseMutualInformation             0.000000e+00  0.000069   -0.313686   \n",
       "MeanConditionalPointwiseMutualInformation -8.007145e-06  0.000008    0.018182   \n",
       "PTrue                                     -3.504455e-05  0.000051   -0.759635   \n",
       "PTrueSampling                              0.000000e+00  0.000002   -0.375160   \n",
       "MonteCarloSequenceEntropy                 -5.044093e-06 -0.000003   -0.068054   \n",
       "MonteCarloNormalizedSequenceEntropy        3.145507e-10 -0.000005    0.009059   \n",
       "LexicalSimilarity_rouge1                   0.000000e+00 -0.000023    0.062659   \n",
       "LexicalSimilarity_rouge2                   0.000000e+00 -0.000264   -0.080649   \n",
       "LexicalSimilarity_rougeL                   0.000000e+00 -0.000696    0.072246   \n",
       "LexicalSimilarity_BLEU                     0.000000e+00  0.000064    0.059771   \n",
       "NumSemSets                                 0.000000e+00  0.000000    0.472821   \n",
       "EigValLaplacian_NLI_score_entail           7.814226e-06 -0.000050    0.009214   \n",
       "EigValLaplacian_NLI_score_contra          -2.874865e-04 -0.000353    0.006322   \n",
       "EigValLaplacian_Jaccard_score             -2.516254e-05 -0.000138    0.000668   \n",
       "DegMat_NLI_score_entail                    2.338089e-08 -0.000073    0.013616   \n",
       "DegMat_NLI_score_contra                    0.000000e+00 -0.000023    0.012561   \n",
       "DegMat_Jaccard_score                      -2.526807e-05  0.000104    0.068838   \n",
       "Eccentricity_NLI_score_entail             -1.484132e-04 -0.000091    0.016900   \n",
       "Eccentricity_NLI_score_contra             -3.898841e-04  0.000131    0.011376   \n",
       "Eccentricity_Jaccard_score                 8.248144e-04 -0.001261    0.095425   \n",
       "SemanticEntropy                           -5.721821e-06 -0.000007   -0.023679   \n",
       "\n",
       "                                           Isotonic PCC  \n",
       "MaximumSequenceProbability                -5.721821e-06  \n",
       "Perplexity                                -7.831774e-06  \n",
       "MeanTokenEntropy                           3.145503e-10  \n",
       "MeanPointwiseMutualInformation             2.883105e-08  \n",
       "MeanConditionalPointwiseMutualInformation -7.831774e-06  \n",
       "PTrue                                     -3.782925e-01  \n",
       "PTrueSampling                             -5.205038e-02  \n",
       "MonteCarloSequenceEntropy                 -5.044093e-06  \n",
       "MonteCarloNormalizedSequenceEntropy        3.145507e-10  \n",
       "LexicalSimilarity_rouge1                  -1.029108e-05  \n",
       "LexicalSimilarity_rouge2                  -6.666348e-03  \n",
       "LexicalSimilarity_rougeL                  -8.362908e-04  \n",
       "LexicalSimilarity_BLEU                    -3.438847e-05  \n",
       "NumSemSets                                 0.000000e+00  \n",
       "EigValLaplacian_NLI_score_entail          -2.335574e-06  \n",
       "EigValLaplacian_NLI_score_contra          -1.899861e-04  \n",
       "EigValLaplacian_Jaccard_score              4.344693e-04  \n",
       "DegMat_NLI_score_entail                   -7.053086e-05  \n",
       "DegMat_NLI_score_contra                   -1.891540e-04  \n",
       "DegMat_Jaccard_score                      -3.689794e-05  \n",
       "Eccentricity_NLI_score_entail              1.973985e-04  \n",
       "Eccentricity_NLI_score_contra              7.307482e-04  \n",
       "Eccentricity_Jaccard_score                -1.006939e-03  \n",
       "SemanticEntropy                           -5.724935e-06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_ues, all_gen_metrics = get_mans_ues_metrics(list(EVAL_MAN_PATHS.values()), UE_METHOD_NAMES, GEN_METRIC_NAMES)\n",
    "\n",
    "# Same for all datasets concatenated\n",
    "for metric_name in GEN_METRIC_NAMES:\n",
    "    gen_metrics = all_gen_metrics[metric_name]\n",
    "    for ue_method_name in UE_METHOD_NAMES:\n",
    "        ues = all_ues[ue_method_name]\n",
    "        \n",
    "        filtered_metric, filtered_ues = filter_nans(gen_metrics, ues)\n",
    "        \n",
    "        minmax_ues = -np.array(confidences['min_max'][metric_name][ue_method_name])\n",
    "        quantile_ues = -np.array(confidences['quantile'][metric_name][ue_method_name])\n",
    "        binned_pcc_ues = -np.array(confidences['binned_pcc'][metric_name][ue_method_name])\n",
    "        isotonic_pcc_ues = -np.array(confidences['isotonic_pcc'][metric_name][ue_method_name])\n",
    "\n",
    "        assert(len(filtered_ues) == len(minmax_ues))\n",
    "        \n",
    "        oracle_score = ue_metric(-filtered_metric, filtered_metric)\n",
    "        random_score = get_random_scores(ue_metric, filtered_metric)\n",
    "\n",
    "        raw_ue_metric_val = ue_metric(filtered_ues, filtered_metric)\n",
    "        raw_score = normalize_metric(raw_ue_metric_val, oracle_score, random_score)\n",
    "\n",
    "        minmax_ue_metric_val = ue_metric(minmax_ues, filtered_metric)\n",
    "        minmax_score = normalize_metric(minmax_ue_metric_val, oracle_score, random_score)\n",
    "        minmax_diff = raw_score - minmax_score\n",
    "\n",
    "        quantile_ue_metric_val = ue_metric(quantile_ues, filtered_metric)\n",
    "        quantile_score = normalize_metric(quantile_ue_metric_val, oracle_score, random_score)\n",
    "        quantile_diff = raw_score - quantile_score\n",
    "        \n",
    "        binned_pcc_ue_metric_val = ue_metric(binned_pcc_ues, filtered_metric)\n",
    "        binned_pcc_score = normalize_metric(binned_pcc_ue_metric_val, oracle_score, random_score)\n",
    "        binned_pcc_diff = raw_score - binned_pcc_score\n",
    "\n",
    "        isotonic_pcc_ue_metric_val = ue_metric(isotonic_pcc_ues, filtered_metric)\n",
    "        isotonic_pcc_score = normalize_metric(isotonic_pcc_ue_metric_val, oracle_score, random_score)\n",
    "        isotonic_pcc_diff = raw_score - isotonic_pcc_score\n",
    "\n",
    "        res[ue_method_name] = [minmax_diff, quantile_diff, binned_pcc_diff, isotonic_pcc_diff]\n",
    "\n",
    "df = pd.DataFrame.from_dict(res, orient='index', columns=cols)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
