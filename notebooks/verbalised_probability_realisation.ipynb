{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_auth_token = \"TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bcdc9af0814f9bac1fc44948c84cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "    \n",
    "    :param seed: The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Load the pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = \"cpu\"  # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Set pad_token_id to eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label prob. method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(question, num_samples=10):\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\":  \"Provide your best guess for the following question. Give ONLY the guess, no other words or explanation.\\n\\n\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me an example of Guess.\"},\n",
    "        {\"role\": \"user\", \"content\": \"For example:\\n\\nGuess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\\n\\n\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, now please give me the question.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"The question is: {question}\"},\n",
    "        ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    # tokenisation and answer generation\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    answers = []\n",
    "    for _ in range(num_samples):\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=1.0, top_p=1.0)\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extracting the part of the answer after \"Guess:\"\n",
    "        guess = answer.split(\"Guess:\")[-1].strip()\n",
    "        answers.append(guess)\n",
    "    \n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "def most_frequent_share(answers):\n",
    "    if not answers:\n",
    "        return 0\n",
    "    counter = Counter(answers)\n",
    "    most_common = counter.most_common(1)[0]\n",
    "    most_common_count = most_common[1]\n",
    "    total_count = len(answers)\n",
    "    share = most_common_count / total_count\n",
    "    return share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "question = \"What is the oldest country?\"\n",
    "answers = generate_answers(question, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Egypt.',\n",
       " 'Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)',\n",
       " 'Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)',\n",
       " 'Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)',\n",
       " 'Egypt or Sumeria.',\n",
       " 'Egypt or Sumeria.',\n",
       " 'Egypt.',\n",
       " 'Egypt or Sumeria.',\n",
       " 'Egypt.',\n",
       " 'Egypt or Sumer (Mesopotamia)']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Egypt.', 'Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)'), ('Egypt.', 'Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)'), ('Egypt.', 'Egypt or Sumeria.'), ('Egypt.', 'Egypt or Sumeria.'), ('Egypt.', 'Egypt.'), ('Egypt.', 'Egypt or Sumeria.'), ('Egypt.', 'Egypt.'), ('Egypt.', 'Egypt or Sumer (Mesopotamia)'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt or Sumeria.'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt or Sumeria.'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt.'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt or Sumeria.'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt.'), ('Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)', 'Egypt or Sumer (Mesopotamia)'), ('Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)', 'Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)'), ('Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)', 'Egypt or Sumeria.'), ('Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)', 'Egypt or Sumeria.'), ('Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)', 'Egypt or Sumeria.'), ('Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)', 'Egypt or Sumer (Mesopotamia)'), ('Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)', 'Egypt or Sumeria.'), ('Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)', 'Egypt or Sumeria.'), ('Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)', 'Egypt.'), ('Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)', 'Egypt or Sumeria.'), ('Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)', 'Egypt.'), ('Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)', 'Egypt or Sumer (Mesopotamia)'), ('Egypt or Sumeria.', 'Egypt or Sumeria.'), ('Egypt or Sumeria.', 'Egypt.'), ('Egypt or Sumeria.', 'Egypt or Sumeria.'), ('Egypt or Sumeria.', 'Egypt.'), ('Egypt or Sumeria.', 'Egypt or Sumer (Mesopotamia)'), ('Egypt or Sumeria.', 'Egypt.'), ('Egypt or Sumeria.', 'Egypt or Sumeria.'), ('Egypt or Sumeria.', 'Egypt.'), ('Egypt or Sumeria.', 'Egypt or Sumer (Mesopotamia)'), ('Egypt.', 'Egypt or Sumeria.'), ('Egypt.', 'Egypt.'), ('Egypt.', 'Egypt or Sumer (Mesopotamia)'), ('Egypt or Sumeria.', 'Egypt.'), ('Egypt or Sumeria.', 'Egypt or Sumer (Mesopotamia)'), ('Egypt.', 'Egypt or Sumer (Mesopotamia)')]\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели и токенизатора для семантической схожести гипотез модели\n",
    "tokenizer_semantic_similarity = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model_semantic_similarity = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer_semantic_similarity(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_semantic_similarity(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "def check_equivalence(answers, threshold=0.5):\n",
    "    embeddings = [get_sentence_embedding(answer) for answer in answers]\n",
    "    embeddings = torch.stack(embeddings).numpy()\n",
    "    \n",
    "    # Вычисление косинусного сходства между всеми парами ответов\n",
    "    cosine_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Порог для определения эквивалентности (может быть настроен)\n",
    "    \n",
    "    equivalence = []\n",
    "    for i in range(len(answers)):\n",
    "        for j in range(i + 1, len(answers)):\n",
    "            if cosine_matrix[i][j] > threshold:\n",
    "                equivalence.append((answers[i], answers[j]))\n",
    "    \n",
    "    return equivalence\n",
    "\n",
    "\n",
    "\n",
    "equivalent_answers = check_equivalence(answers)\n",
    "print(equivalent_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Egypt. (Based on the common belief that Egypt is one of the oldest civilizations in the world.)', 'Egypt (based on some theories suggesting civilization in Egypt dates back to around 3100 BCE)', 'Egypt.', 'Egypt or Sumeria.', 'Egypt or Sumer (Mesopotamia)', 'Egypt (based on some historical records suggesting Egypt as one of the oldest civilizations)')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unite_tuples(tuples_list):\n",
    "    result = []\n",
    "    while tuples_list:\n",
    "        current = tuples_list.pop(0)\n",
    "        merged = list(current)\n",
    "        i = 0\n",
    "        while i < len(tuples_list):\n",
    "            if any(item in merged for item in tuples_list[i]):\n",
    "                merged.extend(tuples_list.pop(i))\n",
    "                merged = list(set(merged))  # Удаляем дубликаты\n",
    "                i = 0  # Начинаем проверку заново\n",
    "            else:\n",
    "                i += 1\n",
    "        result.append(tuple(merged))\n",
    "    return result\n",
    "\n",
    "# Пример использования\n",
    "same_semantics_tuples = unite_tuples(equivalent_answers)\n",
    "print(same_semantics_tuples)\n",
    "counts = [len(x) for x in same_semantics_tuples]\n",
    "label_prob = max(counts) / sum(counts)\n",
    "label_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(question, answer):\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\":  \"Question: {question}\\n\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me a proposed answer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Proposed Answer: {answer}\\n\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, now please give me exact inctruction how to evaluate the proposed answer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Is the proposed answer:\\n\\t(A) True\\n or\\n\\t(B) False\\n\\n Give ONLY the '(A) True' or '(B) False', no other words or explanation.\\n\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The proposed answer is:\\n\\n\"},\n",
    "        ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    \n",
    "    # tokenisation and generation\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    final_result = result.split(\"The proposed answer is:\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "def is_true(question, answers, num_samples=10):\n",
    "    results = [check_answer(question, answer) for answer in answers]\n",
    "    return results\n",
    "\n",
    "def normalize_answer(answer):\n",
    "    # drop everything after '(A) True' or '(B) False'\n",
    "    match = re.match(r\"\\((A|B)\\) (True|False)\", answer)\n",
    "    if match:\n",
    "        return f\"({match.group(1)}) {match.group(2)}\"\n",
    "    return answer\n",
    "\n",
    "def count_true_share(results):\n",
    "    # answer normalization\n",
    "    normalized_results = [normalize_answer(result) for result in results]\n",
    "    \n",
    "    # '(A) True' count\n",
    "    true_count = normalized_results.count('(A) True')\n",
    "    \n",
    "    # '(A) True' share count\n",
    "    total_count = len(normalized_results)\n",
    "    true_share = true_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    return true_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "question = \"What is the oldest country?\"\n",
    "result = is_true(question, answers, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.',\n",
       " '(A) True if the question is clear and the answer is correct, based on the provided context.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(A) True',\n",
       " '(A) True',\n",
       " '(A) True',\n",
       " '(A) True',\n",
       " '(A) True',\n",
       " '(A) True',\n",
       " '(A) True',\n",
       " '(A) True',\n",
       " '(A) True',\n",
       " '(A) True']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_is_true_result = [normalize_answer(x) for x in result]\n",
    "final_is_true_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_share = count_true_share(final_is_true_result)\n",
    "true_share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalised probability methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"survey-results.csv\"\n",
    "# taken from https://waf.cs.illinois.edu/visualizations/Perception-of-Probability-Words/\n",
    "\n",
    "words_probs = pd.read_csv(data_path, delimiter=\",\")\n",
    "words_probs.rename(columns=lambda x: x.replace('\"', ''), inplace=True)\n",
    "words_probs_probs = words_probs.loc[:,'Almost Certain':'Chances are Slight'].median(axis=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_probs_probs = {'Almost Certain': 95.0,\n",
    " 'Highly Likely': 90.0,\n",
    " 'Very Good Chance': 80.0,\n",
    " 'Probable': 70.0,\n",
    " 'Likely': 70.0,\n",
    " 'We Believe': 75.0,\n",
    " 'Probably': 70.0,\n",
    " 'Better than Even': 60.0,\n",
    " 'About Even': 50.0,\n",
    " 'We Doubt': 20.0,\n",
    " 'Improbable': 10.0,\n",
    " 'Unlikely': 20.0,\n",
    " 'Probably Not': 25.0,\n",
    " 'Little Chance': 10.0,\n",
    " 'Almost No Chance': 2.0,\n",
    " 'Highly Unlikely': 5.0,\n",
    " 'Chances are Slight': 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Установка pad_token в eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_text(prompt, max_length=800):\n",
    "    # Токенизация с указанием attention_mask\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Установка pad_token_id в eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Генерация текста с использованием attention_mask\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids, \n",
    "        attention_mask=inputs.attention_mask, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=model.config.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# 3. Verbalized 1-Stage Top-1 (`Verb. 1S top-1`)\n",
    "def verb_1s_top1_prompt(question):\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\":  \"Provide your best guess and the probability that it is correct (0.0 to 1.0) for the following question. Give ONLY the guess and probability, no other words or explanation.\\n\\n\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me an example.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\\nProbability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, now please give me the question.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"The question is: {question}\\n\"},\n",
    "        ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return generate_text(prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Verbalized 1-Stage Top-k (`Verb. 1S top-k`)\n",
    "def verb_1s_topk_prompt(question, k):\n",
    "    \n",
    "    \n",
    "    content_1_user = f\"Provide your {k} best guesses and the probability that each is correct (0.0 to 1.0) for the following question. Give ONLY the guesses and probabilities, no other words or explanation.\\n\\nFor example:\\n\\n\"\n",
    "\n",
    "    content_2_user = \"\" \n",
    "    for i in range(1, k+1):\n",
    "        content_2_user += f\"G{i}: <{i}-th most likely guess, as short as possible; not a complete sentence, just the guess!>\\nP{i}: <the probability between 0.0 and 1.0 that G{i} is correct, without any extra commentary whatsoever; just the probability!>\\n\\n\"\n",
    "    \n",
    "    content_3_user = f\"The question is: {question}\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": content_1_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me an example.\"},\n",
    "        {\"role\": \"user\", \"content\": content_2_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, now please give me the question.\"},\n",
    "        {\"role\": \"user\", \"content\": content_3_user},\n",
    "        ]\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    \n",
    "    return generate_text(prompt)\n",
    "\n",
    "# 3. Verbalized 1-Stage Top-1 (`Verb. 1S top-1`)\n",
    "def verb_2s_top1_prompt(question):\n",
    "    # prompt = f\"\"\"\n",
    "    \n",
    "    \n",
    "    # For example:\n",
    "    \n",
    "    # Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
    "    # The question is: {question}\n",
    "    # Provide the probability that your guess is correct. Give ONLY the probability, no\n",
    "    # other words or explanation.\n",
    "    \n",
    "    # For example:\n",
    "    \n",
    "    # Probability: <the probability between\n",
    "    # 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever;\n",
    "    # just the probability!>\n",
    "    \n",
    "    \n",
    "    # \"\"\"\n",
    "\n",
    "    content_1_user = f\"Provide your best guess for the following question. Give ONLY the guess, no other words or explanation.\\n\\n\"\n",
    "\n",
    "    content_2_user = f\"For example:\\n\\n\\nGuess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\\nThe question is: {question}\\n\\nProvide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\" \n",
    "    \n",
    "    content_3_user = \"For example:\\n\\n\\nProbability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": content_1_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me an example.\"},\n",
    "        {\"role\": \"user\", \"content\": content_2_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, please give me the instructions about probability.\"},\n",
    "        {\"role\": \"user\", \"content\": content_3_user},\n",
    "        ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return generate_text(prompt)\n",
    "\n",
    "# 4. Verbalized 2-Stage Top-k (`Verb. 2S top-k`)\n",
    "def verb_2s_topk_prompt(question, k):\n",
    "    content_1_user = f\"Provide your {k} best guess for the following question. Give ONLY the guess, no other words or explanation.\\n\\nFor example:\\n\\n\"\n",
    "\n",
    "    content_2_user = \"\"\n",
    "    for i in range(1, k+1):\n",
    "        content_2_user += f\"G{i}: <{i}-th most likely guess, as short as possible; not a complete sentence, just the guess!>\\n\"\n",
    "        content_2_user += f\"P{i}: <the probability between 0.0 and 1.0 that G{i} is correct, without any extra commentary whatsoever; just the probability!>\\n\\n\"\n",
    "    \n",
    "    content_3_user = f\"The question is: {question}\\n\"\n",
    "\n",
    "    content_4_user = \"Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\\n\\nFor example:\\n\\n\"\n",
    "    for i in range(1, k+1):\n",
    "        content_4_user += f\"P{i}: <the probability between 0.0 and 1.0 that G{i} is correct, without any extra commentary whatsoever; just the probability!>\\n\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": content_1_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me an example.\"},\n",
    "        {\"role\": \"user\", \"content\": content_2_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, please give me the question.\"},\n",
    "        {\"role\": \"user\", \"content\": content_3_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, please give me the instructions about probability.\"},\n",
    "        {\"role\": \"user\", \"content\": content_4_user},\n",
    "        ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return generate_text(prompt)\n",
    "\n",
    "# 6. Verbalized 2-Stage Chain-of-Thought (`Verb. 2S CoT`)\n",
    "def verb_2s_cot_prompt(question):\n",
    "\n",
    "    content_1_user = \"Provide your best guess for the following question. Before giving your answer, provide a step-by-step explanation of your thought process. Then on a new line give the guess with no other words or explanation.\"\n",
    "    content_2_user = \"For example:\\n\\nExplanation:<one sentence step-by-step explanation of your thought process>\\n\\nGuess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\"\n",
    "    content_3_user = f\"The question is: {question}\"\n",
    "    content_4_user = \"Provide the probability that your previous guess is correct (0.0 to 1.0). Give ONLY the probability, no other words or explanation.\\n\\nFor example:\\n\\nProbability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": content_1_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me an example.\"},\n",
    "        {\"role\": \"user\", \"content\": content_2_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, please give me the question.\"},\n",
    "        {\"role\": \"user\", \"content\": content_3_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, please give me the instructions about probability.\"},\n",
    "        {\"role\": \"user\", \"content\": content_4_user},\n",
    "        ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return generate_text(prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Linguistic 1-Stage Human (`Ling. 1S-human`)\n",
    "def ling_1s_human_prompt(question):\n",
    "    \n",
    "    content_1_user = \"Provide your best guess and the likelihood that it is correct for the following question, using one of the following expressions: {{'Almost Certain', 'Highly Likely', 'Very Good Chance', 'Probable', 'Likely', 'We Believe', 'Probably', 'Better than Even', 'About Even', 'We Doubt', 'Improbable', 'Unlikely', 'Probably Not', 'Little Chance', 'Almost No Chance', 'Highly Unlikely', 'Chances are Slight'}}. Give ONLY the guess and likelihood, no other words or explanation.\"\n",
    "    content_2_user = \"For example:\\n\\nGuess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\\nLikelihood: <one of the expressions from the list>\"\n",
    "    content_3_user = f\"The question is: {question}\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": content_1_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okey, please, give me an example.\"},\n",
    "        {\"role\": \"user\", \"content\": content_2_user},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you, please give me the question.\"},\n",
    "        {\"role\": \"user\", \"content\": content_3_user},\n",
    "        ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return generate_text(prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: What is the capital of Russia?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"What is the capital of Russia?\"\n",
    "print(\"Sample Question:\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 1-Stage Top-1 Method Output:\n",
      "[INST] Provide your best guess and the probability that it is correct (0.0 to 1.0) for the following question. Give ONLY the guess and probability, no other words or explanation.\n",
      "\n",
      " [/INST] Okey, please, give me an example. [INST] Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!> [/INST] Thank you, now please give me the question. [INST] The question is: What is the capital of Russia?\n",
      " [/INST] Guess: Moscow\n",
      "Probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 1-Stage Top-1 Method Output:\")\n",
    "print(verb_1s_top1_prompt(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 1-Stage Top-2 Method Output:\n",
      "[INST] Provide your 2 best guesses and the probability that each is correct (0.0 to 1.0) for the following question. Give ONLY the guesses and probabilities, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      " [/INST] Okey, please, give me an example. [INST] G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      " [/INST] Thank you, now please give me the question. [INST] The question is: What is the capital of Russia? [/INST] G1: Moscow\n",
      "P1: 0.95\n",
      "\n",
      "G2: Saint Petersburg\n",
      "P2: 0.05\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 1-Stage Top-2 Method Output:\")\n",
    "print(verb_1s_topk_prompt(question, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 1-Stage Top-4 Method Output:\n",
      "[INST] Provide your 4 best guesses and the probability that each is correct (0.0 to 1.0) for the following question. Give ONLY the guesses and probabilities, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      " [/INST] Okey, please, give me an example. [INST] G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G3: <3-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P3: <the probability between 0.0 and 1.0 that G3 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G4: <4-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P4: <the probability between 0.0 and 1.0 that G4 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      " [/INST] Thank you, now please give me the question. [INST] The question is: What is the capital of Russia? [/INST] G1: Moscow\n",
      "P1: 0.9\n",
      "\n",
      "G2: Saint Petersburg\n",
      "P2: 0.1\n",
      "\n",
      "G3: Kazan\n",
      "P3: 0.03\n",
      "\n",
      "G4: Novosibirsk\n",
      "P4: 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 1-Stage Top-4 Method Output:\")\n",
    "print(verb_1s_topk_prompt(question, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Top-1 Method Output:\n",
      "[INST] Provide your 1 best guess for the following question. Give ONLY the guess, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      " [/INST] Okey, please, give me an example. [INST] G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      " [/INST] Thank you, please give me the question. [INST] The question is: What is the capital of Russia?\n",
      " [/INST] Thank you, please give me the instructions about probability. [INST] Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      " [/INST] G1: Moscow\n",
      " P1: 1.0\n",
      "\n",
      "(Note: The probability 1.0 indicates a certain answer, in this case, the capital of Russia is Moscow and it is a certain fact.)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Top-1 Method Output:\")\n",
    "print(verb_2s_topk_prompt(question, 1))\n",
    "#print(\"Guesses:\", guesses)\n",
    "#print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Top-2 Method Output:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Provide your 2 best guess for the following question. Give ONLY the guess, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      " [/INST] Okey, please, give me an example. [INST] G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      " [/INST] Thank you, please give me the question. [INST] The question is: What is the capital of Russia?\n",
      " [/INST] Thank you, please give me the instructions about probability. [INST] Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      " [/INST] G1: Moscow\n",
      "P1: 0.999\n",
      "\n",
      "G2: Saint Petersburg\n",
      "P2: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Top-2 Method Output:\")\n",
    "print(verb_2s_topk_prompt(question, 2))\n",
    "#print(\"Guesses:\", guesses)\n",
    "#print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Top-4 Method Output:\n",
      "[INST] Provide your 4 best guess for the following question. Give ONLY the guess, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      " [/INST] Okey, please, give me an example. [INST] G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G3: <3-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P3: <the probability between 0.0 and 1.0 that G3 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G4: <4-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P4: <the probability between 0.0 and 1.0 that G4 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      " [/INST] Thank you, please give me the question. [INST] The question is: What is the capital of Russia?\n",
      " [/INST] Thank you, please give me the instructions about probability. [INST] Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P3: <the probability between 0.0 and 1.0 that G3 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P4: <the probability between 0.0 and 1.0 that G4 is correct, without any extra commentary whatsoever; just the probability!>\n",
      " [/INST] P1: 1.0\n",
      "G1: Moscow\n",
      "\n",
      "P2: 0.0\n",
      "G2: Paris\n",
      "\n",
      "P3: 0.0\n",
      "G3: London\n",
      "\n",
      "P4: 0.0\n",
      "G4: New York\n",
      "\n",
      "(Note: The capital of Russia is Moscow, so the probability of guessing it correctly is 100%.)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Top-4 Method Output:\")\n",
    "print(verb_2s_topk_prompt(question, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Chain-of-Thought Method Output:\n",
      "[INST] Provide your best guess for the following question. Before giving your answer, provide a step-by-step explanation of your thought process. Then on a new line give the guess with no other words or explanation. [/INST] Okey, please, give me an example. [INST] For example:\n",
      "\n",
      "Explanation:<one sentence step-by-step explanation of your thought process>\n",
      "\n",
      "Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!> [/INST] Thank you, please give me the question. [INST] The question is: What is the capital of Russia? [/INST] Thank you, please give me the instructions about probability. [INST] Provide the probability that your previous guess is correct (0.0 to 1.0). Give ONLY the probability, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!> [/INST] Probability: 1.0\n",
      "\n",
      "Explanation: The question asked for the capital of Russia, and my guess was Moscow, which is indeed the correct answer. Therefore, the probability of my guess being correct is 1.0.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Chain-of-Thought Method Output:\")\n",
    "print(verb_2s_cot_prompt(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linguistic 1-Stage Human Method Output:\n",
      "[INST] Provide your best guess and the likelihood that it is correct for the following question, using one of the following expressions: {{'Almost Certain', 'Highly Likely', 'Very Good Chance', 'Probable', 'Likely', 'We Believe', 'Probably', 'Better than Even', 'About Even', 'We Doubt', 'Improbable', 'Unlikely', 'Probably Not', 'Little Chance', 'Almost No Chance', 'Highly Unlikely', 'Chances are Slight'}}. Give ONLY the guess and likelihood, no other words or explanation. [/INST] Okey, please, give me an example. [INST] For example:\n",
      "\n",
      "Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "Likelihood: <one of the expressions from the list> [/INST] Thank you, please give me the question. [INST] The question is: What is the capital of Russia? [/INST] Guess: Moscow\n",
      "Likelihood: Almost Certain\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nLinguistic 1-Stage Human Method Output:\")\n",
    "\n",
    "output_ling_1s_human = ling_1s_human_prompt(question)\n",
    "print(output_ling_1s_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ling_1s_human_list = output_ling_1s_human.split(\"\\n\")\n",
    "answer = output_ling_1s_human_list[-1].split(\"Likelihood:\")[-1]\n",
    "proba = words_probs_probs[output_ling_1s_human_list[-1].split(\": \")[1]] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' Almost Certain', 0.95)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
