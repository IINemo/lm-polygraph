{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_auth_token = \"TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d487da419544fbbb29e0c3bc3562cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "    \n",
    "    :param seed: The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Load the pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = \"cpu\"  # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Set pad_token_id to eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label prob. method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prob_method(model, tokenizer, question, device='cpu', n_samples=10):\n",
    "    \"\"\"\n",
    "    Method for estimating the probability of the most likely answer to a question using the Label Prob. method.\n",
    "    \n",
    "    :param model: The model used for generating answers.\n",
    "    :param tokenizer: Tokenizer for converting text to tokens.\n",
    "    :param question: The question to be answered.\n",
    "    :param device: The device (CPU or GPU) on which the model and tokenizer are running.\n",
    "    :param n_samples: The number of samples to generate for probability estimation.\n",
    "    :param seed: The random seed for reproducibility.\n",
    "    :return: The most likely answer and its average probability.\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "        \n",
    "    # Formulate the question template\n",
    "    prompt = f\"Provide your best guess for the following question. Give ONLY the guess, no other words or explanation.\\n\\nFor example:\\n\\nGuess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\\n\\nThe question is: {question}\"\n",
    "\n",
    "    # Tokenize the question\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs.input_ids.shape[1]  # Length of input tokens\n",
    "\n",
    "    # Generate n_samples answers to the question\n",
    "    answers = []\n",
    "    log_probabilities = []\n",
    "    for j in range(n_samples):\n",
    "        with torch.no_grad():\n",
    "            set_seed(seed=j)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=input_length + 50,  # Allow room for the generated answer\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Extract generated tokens and logits\n",
    "        generated_tokens = outputs.sequences[0][input_length:]  # Remove input tokens\n",
    "        scores = outputs.scores[input_length-1:]  # Remove scores corresponding to input tokens\n",
    "        \n",
    "        # Вычисление логарифмов вероятностей для каждого токена\n",
    "        token_log_probabilities = []\n",
    "        for i, score in enumerate(scores[input_length-1:]):  # Начало с логитов после входных токенов\n",
    "            token_logits = score[0]\n",
    "            token_prob = F.softmax(token_logits, dim=-1)\n",
    "            token_log_prob = torch.log(token_prob[generated_tokens[i]])\n",
    "            token_log_probabilities.append(token_log_prob.item())\n",
    "            \n",
    "            # Прерывание, если встречен токен конца предложения\n",
    "            if generated_tokens[i] == tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        # Вычисление логарифма вероятности всего ответа\n",
    "        sequence_log_probability = sum(token_log_probabilities)\n",
    "        log_probabilities.append(sequence_log_probability)\n",
    "        \n",
    "        # Декодирование сгенерированного ответа\n",
    "        answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "    # Подсчет наиболее частого ответа\n",
    "    most_common_answer, most_common_count = Counter(answers).most_common(1)[0]\n",
    "\n",
    "    # Вычисление средней логарифмической вероятности наиболее частого ответа\n",
    "    most_common_log_probabilities = [log_prob for ans, log_prob in zip(answers, log_probabilities) if ans == most_common_answer]\n",
    "    average_log_probability = sum(most_common_log_probabilities) / len(most_common_log_probabilities)\n",
    "    \n",
    "    # Convert average log probability back to probability\n",
    "    average_probability = torch.exp(torch.tensor(average_log_probability)).item()\n",
    "\n",
    "    return most_common_answer, average_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common answer: Guess: Happiness, love, self-realization, or spiritual growth.\n",
      "Average probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "question = \"What is the capital of Russia?\"\n",
    "most_common_answer, average_probability = label_prob_method(model, tokenizer, question, n_samples=2)\n",
    "print(f\"Most common answer: {most_common_answer}\")\n",
    "print(f\"Average probability: {average_probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, question):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=50)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "def is_true_prob_prompt(model, tokenizer, question):\n",
    "    answer = generate_answer(model, tokenizer, question)\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Proposed Answer: {answer}\n",
    "    Is the proposed answer:\n",
    "      (A) True or\n",
    "      (B) False?\n",
    "    \n",
    "    The proposed answer is:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_probabilities(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    true_token_id = tokenizer.convert_tokens_to_ids(\"True\")\n",
    "    false_token_id = tokenizer.convert_tokens_to_ids(\"False\")\n",
    "    \n",
    "    true_prob = probs[0, -1, true_token_id].item()\n",
    "    false_prob = probs[0, -1, false_token_id].item()\n",
    "    \n",
    "    return {\"True\": true_prob, \"False\": false_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True': 1.030938747703658e-07, 'False': 9.919518539902583e-10}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"What is the capital of Russia?\"\n",
    "prompt = is_true_prob_prompt(model, tokenizer, question)\n",
    "probabilities = get_probabilities(model, tokenizer, prompt)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalised probability methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"survey-results.csv\"\n",
    "# taken from https://waf.cs.illinois.edu/visualizations/Perception-of-Probability-Words/\n",
    "\n",
    "words_probs = pd.read_csv(data_path, delimiter=\",\")\n",
    "words_probs.rename(columns=lambda x: x.replace('\"', ''), inplace=True)\n",
    "words_probs_probs = words_probs.loc[:,'Almost Certain':'Chances are Slight'].median(axis=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_probs_probs = {'Almost Certain': 95.0,\n",
    " 'Highly Likely': 90.0,\n",
    " 'Very Good Chance': 80.0,\n",
    " 'Probable': 70.0,\n",
    " 'Likely': 70.0,\n",
    " 'We Believe': 75.0,\n",
    " 'Probably': 70.0,\n",
    " 'Better than Even': 60.0,\n",
    " 'About Even': 50.0,\n",
    " 'We Doubt': 20.0,\n",
    " 'Improbable': 10.0,\n",
    " 'Unlikely': 20.0,\n",
    " 'Probably Not': 25.0,\n",
    " 'Little Chance': 10.0,\n",
    " 'Almost No Chance': 2.0,\n",
    " 'Highly Unlikely': 5.0,\n",
    " 'Chances are Slight': 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Установка pad_token в eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_text(prompt, max_length=800):\n",
    "    # Токенизация с указанием attention_mask\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Установка pad_token_id в eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Генерация текста с использованием attention_mask\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids, \n",
    "        attention_mask=inputs.attention_mask, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=model.config.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# 3. Verbalized 1-Stage Top-1 (`Verb. 1S top-1`)\n",
    "def verb_1s_top1_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    Provide your best guess and the probability that it is correct (0.0 to 1.0) for the following question. Give ONLY the guess and probability, no other words or explanation.\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "    Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
    "    Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>\n",
    "    \n",
    "    The question is: {question}\n",
    "    \"\"\"\n",
    "    return generate_text(prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Verbalized 1-Stage Top-k (`Verb. 1S top-k`)\n",
    "def verb_1s_topk_prompt(question, k):\n",
    "    prompt = f\"Provide your {k} best guesses and the probability that each is correct (0.0 to 1.0) for the following question. Give ONLY the guesses and probabilities, no other words or explanation.\\n\\nFor example:\\n\\n\"\n",
    "    for i in range(1, k+1):\n",
    "        prompt += f\"G{i}: <{i}-th most likely guess, as short as possible; not a complete sentence, just the guess!>\\nP{i}: <the probability between 0.0 and 1.0 that G{i} is correct, without any extra commentary whatsoever; just the probability!>\\n\\n\"\n",
    "    prompt += f\"The question is: {question}\"\n",
    "    return generate_text(prompt)\n",
    "\n",
    "# 3. Verbalized 1-Stage Top-1 (`Verb. 1S top-1`)\n",
    "def verb_2s_top1_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    Provide your best guess for the following question. Give ONLY the guess, no other words or explanation.\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "    Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
    "    The question is: {question}\n",
    "    Provide the probability that your guess is correct. Give ONLY the probability, no\n",
    "    other words or explanation.\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "    Probability: <the probability between\n",
    "    0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever;\n",
    "    just the probability!>\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    return generate_text(prompt)\n",
    "\n",
    "# 4. Verbalized 2-Stage Top-k (`Verb. 2S top-k`)\n",
    "def verb_2s_topk_prompt(question, k):\n",
    "    prompt = f\"Provide your {k} best guess for the following question. Give ONLY the guess, no other words or explanation.\\n\\nFor example:\\n\\n\"\n",
    "    for i in range(1, k+1):\n",
    "        prompt += f\"G{i}: <{i}-th most likely guess, as short as possible; not a complete sentence, just the guess!>\\n\"\n",
    "        prompt += f\"P{i}: <the probability between 0.0 and 1.0 that G{i} is correct, without any extra commentary whatsoever; just the probability!>\\n\\n\"\n",
    "    prompt += f\"The question is: {question}\\n\"\n",
    "    prompt += \"Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\\n\\nFor example:\\n\\n\"\n",
    "    for i in range(1, k+1):\n",
    "        prompt += f\"P{i}: <the probability between 0.0 and 1.0 that G{i} is correct, without any extra commentary whatsoever; just the probability!>\\n\"\n",
    "    \n",
    "    return generate_text(prompt)\n",
    "\n",
    "# 6. Verbalized 2-Stage Chain-of-Thought (`Verb. 2S CoT`)\n",
    "def verb_2s_cot_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    Provide your best guess for the following question. Before giving your answer, provide a step-by-step explanation of your thought process. Then on a new line give the guess with no other words or explanation.\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "    Explanation:<one sentence step-by-step explanation of your thought process>\n",
    "\n",
    "    Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
    "    \n",
    "    The question is: {question}\n",
    "\n",
    "    Provide the probability that your previous guess is correct (0.0 to 1.0). Give ONLY the probability, no other words or explanation.\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "    Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>\n",
    "    \"\"\"\n",
    "    return generate_text(prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Linguistic 1-Stage Human (`Ling. 1S-human`)\n",
    "def ling_1s_human_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    Provide your best guess and the likelihood that it is correct for the following question, using one of the following expressions: {{'Almost Certain', 'Highly Likely', 'Very Good Chance', 'Probable', 'Likely', 'We Believe', 'Probably', 'Better than Even', 'About Even', 'We Doubt', 'Improbable', 'Unlikely', 'Probably Not', 'Little Chance', 'Almost No Chance', 'Highly Unlikely', 'Chances are Slight'}}. Give ONLY the guess and likelihood, no other words or explanation.\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "    Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
    "    Likelihood: <one of the expressions from the list>\n",
    "    \n",
    "    The question is: {question}\n",
    "    \"\"\"\n",
    "    return generate_text(prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: What is the capital of Russia?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"What is the capital of Russia?\"\n",
    "print(\"Sample Question:\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 1-Stage Top-1 Method Output:\n",
      "\n",
      "    Provide your best guess and the probability that it is correct (0.0 to 1.0) for the following question. Give ONLY the guess and probability, no other words or explanation.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "    Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>\n",
      "    \n",
      "    The question is: What is the capital of Russia?\n",
      "    \n",
      "    Guess: Moscow\n",
      "    Probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 1-Stage Top-1 Method Output:\")\n",
    "print(verb_1s_top1_prompt(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 1-Stage Top-2 Method Output:\n",
      "Provide your 2 best guesses and the probability that each is correct (0.0 to 1.0) for the following question. Give ONLY the guesses and probabilities, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "The question is: What is the capital of Russia?\n",
      "\n",
      "G1: Kiev\n",
      "P1: 0.0\n",
      "\n",
      "G2: Moscow\n",
      "P2: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 1-Stage Top-2 Method Output:\")\n",
    "print(verb_1s_topk_prompt(question, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 1-Stage Top-4 Method Output:\n",
      "Provide your 4 best guesses and the probability that each is correct (0.0 to 1.0) for the following question. Give ONLY the guesses and probabilities, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G3: <3-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P3: <the probability between 0.0 and 1.0 that G3 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G4: <4-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P4: <the probability between 0.0 and 1.0 that G4 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "The question is: What is the capital of Russia?\n",
      "\n",
      "G1: Paris\n",
      "P1: 0.0\n",
      "\n",
      "G2: London\n",
      "P2: 0.0\n",
      "\n",
      "G3: Berlin\n",
      "P3: 0.0\n",
      "\n",
      "G4: Moscow\n",
      "P4: 0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 1-Stage Top-4 Method Output:\")\n",
    "print(verb_1s_topk_prompt(question, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Top-1 Method Output:\n",
      "Provide your 1 best guess for the following question. Give ONLY the guess, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "The question is: What is the capital of Russia?\n",
      "Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "G1: Moscow\n",
      "P1: 1.0\n",
      "\n",
      "Answer:\n",
      "\n",
      "P1: 1.0\n",
      "G1: Moscow\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Top-1 Method Output:\")\n",
    "print(verb_2s_topk_prompt(question, 1))\n",
    "#print(\"Guesses:\", guesses)\n",
    "#print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Top-2 Method Output:\n",
      "Provide your 2 best guess for the following question. Give ONLY the guess, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "The question is: What is the capital of Russia?\n",
      "Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G1: Moscow\n",
      "P1: 1.0\n",
      "\n",
      "G2: St. Petersburg\n",
      "P2: 0.1\n",
      "\n",
      "(Note: The probabilities are just for the purpose of this exercise and do not reflect any real-world knowledge or information.)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Top-2 Method Output:\")\n",
    "print(verb_2s_topk_prompt(question, 2))\n",
    "#print(\"Guesses:\", guesses)\n",
    "#print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Top-4 Method Output:\n",
      "Provide your 4 best guess for the following question. Give ONLY the guess, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "G1: <1-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G2: <2-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G3: <3-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P3: <the probability between 0.0 and 1.0 that G3 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G4: <4-th most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "P4: <the probability between 0.0 and 1.0 that G4 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "The question is: What is the capital of Russia?\n",
      "Provide the probability that your guess is correct. Give ONLY the probability, no other words or explanation.\n",
      "\n",
      "For example:\n",
      "\n",
      "P1: <the probability between 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P2: <the probability between 0.0 and 1.0 that G2 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P3: <the probability between 0.0 and 1.0 that G3 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "P4: <the probability between 0.0 and 1.0 that G4 is correct, without any extra commentary whatsoever; just the probability!>\n",
      "\n",
      "G1: Moscow\n",
      "P1: 1.0\n",
      "\n",
      "G2: St. Petersburg\n",
      "P2: 0.1\n",
      "\n",
      "G3: Kiev\n",
      "P3: 0.01\n",
      "\n",
      "G4: Leningrad\n",
      "P4: 0.001\n",
      "\n",
      "So, the answer would look like this:\n",
      "\n",
      "P1: 1.0\n",
      "P2: 0.1\n",
      "P3: 0.01\n",
      "P4: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Top-4 Method Output:\")\n",
    "print(verb_2s_topk_prompt(question, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verbalized 2-Stage Chain-of-Thought Method Output:\n",
      "\n",
      "    Provide your best guess for the following question. Before giving your answer, provide a step-by-step explanation of your thought process. Then on a new line give the guess with no other words or explanation.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    Explanation:<one sentence step-by-step explanation of your thought process>\n",
      "\n",
      "    Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "    \n",
      "    The question is: What is the capital of Russia?\n",
      "\n",
      "    Provide the probability that your previous guess is correct (0.0 to 1.0). Give ONLY the probability, no other words or explanation.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>\n",
      "    \n",
      "    Explanation: Russia is located in Europe and Asia, so its capital could be Moscow or St. Petersburg, both of which are major cities in Russia. However, Moscow is the larger city and has been the capital for a longer period of time, so it is the more likely answer.\n",
      "\n",
      "    Guess: Moscow\n",
      "\n",
      "    Probability: 0.8\n",
      "\n",
      "    Explanation: The capital of Russia is Moscow.\n",
      "\n",
      "    Guess: Moscow\n",
      "\n",
      "    Probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerbalized 2-Stage Chain-of-Thought Method Output:\")\n",
    "print(verb_2s_cot_prompt(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linguistic 1-Stage Human Method Output:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Provide your best guess and the likelihood that it is correct for the following question, using one of the following expressions: {'Almost Certain', 'Highly Likely', 'Very Good Chance', 'Probable', 'Likely', 'We Believe', 'Probably', 'Better than Even', 'About Even', 'We Doubt', 'Improbable', 'Unlikely', 'Probably Not', 'Little Chance', 'Almost No Chance', 'Highly Unlikely', 'Chances are Slight'}. Give ONLY the guess and likelihood, no other words or explanation.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
      "    Likelihood: <one of the expressions from the list>\n",
      "    \n",
      "    The question is: What is the capital of Russia?\n",
      "    \n",
      "    Guess: Moscow\n",
      "    Likelihood: Almost Certain\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nLinguistic 1-Stage Human Method Output:\")\n",
    "\n",
    "output_ling_1s_human = ling_1s_human_prompt(question)\n",
    "print(output_ling_1s_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ling_1s_human_list = output_ling_1s_human.split(\"\\n\")\n",
    "answer = output_ling_1s_human_list[-2].split(\": \")[1]\n",
    "proba = words_probs_probs[output_ling_1s_human_list[-1].split(\": \")[1]] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Moscow', 0.95)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
