hydra:
  run:
    dir: ${cache_path}/${task}/${model}/${dataset}/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - model: bloomz-560m
  - estimators: default_estimators
  - stat_calculators: default_calculators
  - _self_

cache_path: ./workdir/output
save_path: '${hydra:run.dir}'
instruct: false
task: qa

# Direct loading from HuggingFace - the dataset loader will automatically format
# question + options as input and use answer_idx as target
# The dataset has columns: question, options (dict with A/B/C/D), answer_idx (A/B/C/D), answer
dataset: GBaker/MedQA-USMLE-4-options
text_column: question  # Used to detect MedQA format, actual input will be formatted question + options
label_column: answer_idx  # The correct answer (A, B, C, or D) - this is what we compare against
train_split: train
eval_split: test
max_new_tokens: 3  # Just need to output A, B, C, or D
load_from_disk: false
size: 100  # Use a smaller subset for testing, set to null for full dataset
generation_params:
  stop_strings:
    - "\n"

# Optional: Custom prompt template for formatting question and options
# If not provided, default format will be used:
# "Q: {question}\nA. {option_a}\nB. {option_b}\nC. {option_c}\nD. {option_d}\n"
prompt: "Q:{question}\nA. {option_a}\nB. {option_b}\nC. {option_c}\nD. {option_d}\nAnswer:"

# Optional: Description to prepend to each question
description: "The following are multiple choice questions (with answers) about medical knowledge."

subsample_eval_dataset: -1

# Use Accuracy metric to compare LLM output with answer_idx
generation_metrics:
  - name: AccuracyMetric

ignore_exceptions: false

batch_size: 1

seed:
    - 1

