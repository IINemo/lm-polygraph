hydra:
  run:
    dir: ${cache_path}/${task}/${model.path}/${dataset}/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - estimators: default_TopologicalDivergence

  - stat_calculators: default_TrainMTopDivCalculator
  - base_processing_coqa
  - _self_
model: 
  path: Qwen/Qwen2.5-0.5B-Instruct
  type: CausalLM
  path_to_load_script: model/default_causal.py
  load_model_args:
    device_map: auto
  load_tokenizer_args: {}

cache_path: ./workdir/output
save_path: '${hydra:run.dir}'

task: qa
instruct: false

dataset: ['LM-polygraph/coqa', 'continuation']
text_column: input
label_column: output
train_split: train
eval_split: test
n_shot: 0
few_shot_split: train
few_shot_prompt: null
trust_remote_code: false
size: null

max_new_tokens: 20
load_from_disk: false
generation_params:
  generate_until:
    - "\n"
subsample_eval_dataset: -1
batch_size: 1

generation_metrics: null
ignore_exceptions: false
seed:
    - 1

model_heads_cache: "../../../model_heads_cache.yaml"
max_heads: 6
n_jobs: -1
train_dataset:
  train_data_path: "../../../coqa_Meta-Llama-3-8B-Instruct.csv"
  context_column: context
  question_column: question
  prompt_column: prompt
  response_column: generated_answer
  label_column: hallucination
  subsample: 100
