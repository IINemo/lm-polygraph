{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733b31d9",
   "metadata": {},
   "source": [
    "# Low-Level Examples \n",
    "Here we present low-level examples of integrating LM-Polygraph into LLM inference using HF library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025e26e-fd7f-44b6-88d7-5876439a5ab0",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7111f938-bc8c-4b82-82a1-fce490bc8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = \"cuda:0\"\n",
    "dataset_name = \"LM-Polygraph/triviaqa\"\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024e188",
   "metadata": {},
   "source": [
    "## Sequence-Level Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b10d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many fingers on a coala's foot?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Yesterday?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Кто спел песню Кукла Колдуна?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False) for m in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e515bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.stat_calculators.infer_causal_lm_calculator import InferCausalLMCalculator\n",
    "from lm_polygraph.stat_calculators.greedy_alternatives_nli import GreedyAlternativesNLICalculator\n",
    "from lm_polygraph.estimators.claim_conditioned_probability import ClaimConditionedProbability\n",
    "from lm_polygraph.utils.deberta import Deberta\n",
    "from lm_polygraph.model_adapters import WhiteboxModelBasic\n",
    "\n",
    "\n",
    "max_new_tokens = 50\n",
    "generation_config.temperature = 0.9\n",
    "generation_config.do_sample = True\n",
    "\n",
    "model_adapter = WhiteboxModelBasic(model, tokenizer, tokenizer_args={}, generation_parameters=generation_config)\n",
    "\n",
    "calc_infer_llm = InferCausalLMCalculator(tokenize=False)\n",
    "nli_model = Deberta(device=device)\n",
    "nli_model.setup()\n",
    "calc_nli = GreedyAlternativesNLICalculator(nli_model=nli_model)\n",
    "\n",
    "estimator = ClaimConditionedProbability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec05354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemshelmanov/conda/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: A koala's paws do not have an opposable thumb or digits similar to a human hand. Instead, they have five non-opposable, round pads with sharp claws. These pads help the koala grip\n",
      "Uncertainty score: -0.04141769390181651\n",
      "\n",
      "Output: The song \"Yesterday\" was written and performed by the English band The Beatles. However, it was originally sung solely by Paul McCartney during the recording sessions. The song was released as a solo composition credited to McCartney because\n",
      "Uncertainty score: -0.005671583063792256\n",
      "\n",
      "Output: I'm assuming you're asking who sang the song \"Kukla Koldun\" (Doll by Koldun). Koldun is a Russian singer, and he performed this song at the Eurovision Song Contest 2\n",
      "Uncertainty score: -0.019087387991862634\n",
      "\n",
      "Output: In French, the sentence \"I want a small cup of coffee\" can be translated as \"Je veux une tasse petite de café.\" This sentence structure closely follows the English original, making it easy to remember. \"Je veux\"\n",
      "Uncertainty score: -0.00017718172726055112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "data_loader = DataLoader(chat_messages, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "for batch in data_loader:\n",
    "    encoded = tokenizer(batch, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    deps = {\"model_inputs\": encoded}\n",
    "    deps.update(calc_infer_llm(\n",
    "        deps, texts=batch, model=model_adapter, max_new_tokens=max_new_tokens))\n",
    "    deps.update(calc_nli(deps, texts=None, model=model_adapter))\n",
    "\n",
    "    uncertainty_scores = estimator(deps)\n",
    "    generated_texts = tokenizer.batch_decode(deps['greedy_tokens'])\n",
    "    \n",
    "    for text, ue_score in zip(generated_texts, uncertainty_scores):\n",
    "        print(\"Output:\", text)\n",
    "        print(\"Uncertainty score:\", ue_score)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0beb1",
   "metadata": {},
   "source": [
    "## Claim-Level Examples\n",
    "Here we split text into actomic claims and quantify uncertainty of individual claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c543214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Tell me a bio of Albert Einstein.\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a bio of Alla Pugacheva.\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a bio of Paul McCartney.\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False) for m in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa13a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from lm_polygraph.model_adapters import WhiteboxModelBasic\n",
    "from lm_polygraph.estimators import ClaimConditionedProbabilityClaim\n",
    "from lm_polygraph.stat_calculators import *\n",
    "from lm_polygraph.utils.openai_chat import OpenAIChat\n",
    "from lm_polygraph.utils.deberta import Deberta\n",
    "\n",
    "\n",
    "max_new_tokens = 50\n",
    "generation_config.temperature = 0.9\n",
    "generation_config.do_sample = True\n",
    "\n",
    "model_adapter = WhiteboxModelBasic(model, tokenizer, tokenizer_args={}, generation_parameters=generation_config)\n",
    "\n",
    "calc_infer_llm = InferCausalLMCalculator(tokenize=False)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<Your OpenAI API key>\"\n",
    "calc_claim_extractor = ClaimsExtractor(OpenAIChat(\"gpt-4o\"))\n",
    "\n",
    "calc_claim_nli = GreedyAlternativesNLICalculator(Deberta(device=device))\n",
    "\n",
    "estimator = ClaimConditionedProbabilityClaim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9fb7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Albert Einstein (March 14, 1879 – April 18, 1955) was a theoretical physicist who is widely regarded as one of the greatest scientists in history. Born in Ulm,\n",
      "claim: Albert Einstein was born on March 14, 1879.\n",
      "aligned tokens: [0, 1, 3, 4, 6, 7, 10, 11, 12, 13]\n",
      "UE score: -0.821775516672531\n",
      "claim: Albert Einstein died on April 18, 1955.\n",
      "aligned tokens: [0, 1, 15, 17, 18, 21, 22, 23, 24]\n",
      "UE score: -0.9998953235981506\n",
      "claim: Albert Einstein was a theoretical physicist.\n",
      "aligned tokens: [0, 1, 26, 27, 28, 29, 30, 31]\n",
      "UE score: -0.4184386514851561\n",
      "claim: Albert Einstein is widely regarded as one of the greatest scientists in history.\n",
      "aligned tokens: [0, 1, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]\n",
      "UE score: -0.7276474721926438\n",
      "\n",
      "Output: Alla Pugacheva, born on November 17, 1949, in the Soviet Union, is a legendary Russian singer, songwriter, and television personality. Her full name is Alla Iosifovna P\n",
      "claim: Alla Pugacheva was born on November 17, 1949.\n",
      "aligned tokens: [0, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 15, 16, 17, 18]\n",
      "UE score: -0.578925783372868\n",
      "claim: Alla Pugacheva was born in the Soviet Union.\n",
      "aligned tokens: [0, 1, 2, 3, 4, 5, 7, 20, 21, 22, 23]\n",
      "UE score: -0.009791780621583401\n",
      "claim: Alla Pugacheva is a legendary Russian singer.\n",
      "aligned tokens: [0, 1, 2, 3, 4, 5, 25, 26, 27, 28, 29]\n",
      "UE score: -0.8688035916355697\n",
      "claim: Alla Pugacheva is a songwriter.\n",
      "aligned tokens: [0, 1, 2, 3, 4, 5, 25, 31, 32]\n",
      "UE score: -0.7403110918697617\n",
      "claim: Alla Pugacheva is a television personality.\n",
      "aligned tokens: [0, 1, 2, 3, 4, 5, 25, 26, 35, 36]\n",
      "UE score: -0.11965546693420669\n",
      "\n",
      "Output: Paul McCartney, born on June 18, 1942, in Liverpool, England, is a renowned English musician, singer-songwriter, and composer. He gained international fame as the bass guitarist, primary\n",
      "claim: Paul McCartney was born on June 18, 1942.\n",
      "aligned tokens: [0, 1, 2, 3, 5, 6, 7, 9, 10, 13, 14, 15, 16]\n",
      "UE score: -0.9048994842285542\n",
      "claim: Paul McCartney was born in Liverpool, England.\n",
      "aligned tokens: [0, 1, 2, 3, 5, 18, 19, 21]\n",
      "UE score: -0.9996821599190878\n",
      "claim: Paul McCartney is a renowned English musician.\n",
      "aligned tokens: [0, 1, 2, 3, 23, 24, 25, 26, 27, 28]\n",
      "UE score: -0.5363892277348042\n",
      "claim: Paul McCartney is a renowned singer-songwriter.\n",
      "aligned tokens: [0, 1, 2, 3, 23, 24, 25, 26, 30, 31, 32, 33, 34]\n",
      "UE score: -0.9639304746766358\n",
      "claim: Paul McCartney is a renowned composer.\n",
      "aligned tokens: [0, 1, 2, 3, 23, 24, 25, 26, 37]\n",
      "UE score: -0.9855241019231145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "data_loader = DataLoader(chat_messages, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "for batch in data_loader:\n",
    "    encoded = tokenizer(batch, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    deps = {\"model_inputs\": encoded}\n",
    "    deps.update(calc_infer_llm(\n",
    "        deps, texts=batch, model=model_adapter, max_new_tokens=max_new_tokens))\n",
    "    deps.update({\"greedy_texts\" : tokenizer.batch_decode(deps['greedy_tokens'])})\n",
    "    deps.update(calc_claim_extractor(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_claim_nli(deps, texts=None, model=model_adapter))\n",
    "\n",
    "    uncertainty_scores = estimator(deps)\n",
    "\n",
    "    for text, claims, ue_score in zip(deps[\"greedy_texts\"], deps['claims'], uncertainty_scores):\n",
    "        print(\"Output:\", text)\n",
    "        \n",
    "        for claim, ue in zip(claims, ue_score):\n",
    "            print(\"claim:\", claim.claim_text)\n",
    "            print(\"aligned tokens:\", claim.aligned_token_ids)\n",
    "            print(\"UE score:\", ue)\n",
    "\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
