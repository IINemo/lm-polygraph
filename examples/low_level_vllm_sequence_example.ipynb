{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025e26e-fd7f-44b6-88d7-5876439a5ab0",
   "metadata": {},
   "source": [
    "# Specify HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111f938-bc8c-4b82-82a1-fce490bc8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda:0\"\n",
    "dataset_name = \"../workdir/data/triviaqa.csv\"\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a3862-77d1-4bb4-8423-1f86f3a58b54",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "llm = LLM(model=model_name_or_path, gpu_memory_utilization=0.5)\n",
    "sampling_params = SamplingParams(max_tokens=30, logprobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many fingers on a coala's foot?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Yesterday?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Кто спел песню Кукла Колдуна?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False) for m in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04e238",
   "metadata": {},
   "source": [
    "# Infer LLM and get uncertainty scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541a3db-5ab4-45f1-84ae-3e0a23686906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from lm_polygraph.utils.model import Model\n",
    "from transformers.generation import GenerateDecoderOnlyOutput\n",
    "\n",
    "class WhiteboxModelvLLM(Model):\n",
    "    \"\"\"Basic whitebox model adapter for using vLLM in stat calculators and uncertainty estimators.\"\"\"\n",
    "\n",
    "    def __init__(self, model: LLM, device: str = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.get_tokenizer()\n",
    "        self.base_device = device\n",
    "        self.model_type = \"vLLMCausalLM\"\n",
    "        \n",
    "    def generate(self, *args, **kwargs):\n",
    "        sampling_params = kwargs.pop(\"sampling_params\")\n",
    "        sampling_params.n = kwargs.get(\"num_return_sequences\", 1)\n",
    "        output = self.model.generate(*args, sampling_params)\n",
    "        return self.post_processing(output)\n",
    "\n",
    "    def device(self):\n",
    "        return self.base_device\n",
    "\n",
    "    def tokenize(self, *args, **kwargs):\n",
    "        return self.tokenizer(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.generate(*args, **kwargs)\n",
    "\n",
    "    def generate_texts(self, input_texts: List[str], **args):\n",
    "        outputs = self.generate(input_texts, **args)\n",
    "        texts = [\n",
    "            outputs.text\n",
    "            for sampled_outputs in outputs\n",
    "            for output in sampled_outputs.outputs\n",
    "        ]      \n",
    "        return texts\n",
    "\n",
    "    def post_processing(self, outputs):\n",
    "        \n",
    "        standard_output = GenerateDecoderOnlyOutput()\n",
    "        vocab_size = max(self.tokenizer.vocab_size, max(self.tokenizer.added_tokens_decoder.keys()))\n",
    "        logits = []\n",
    "        sequences = []\n",
    "\n",
    "        max_seq_len = max([\n",
    "            len(output.token_ids)\n",
    "            for sampled_outputs in outputs\n",
    "            for output in sampled_outputs.outputs\n",
    "        ])\n",
    "        for sample_output in outputs:\n",
    "\n",
    "            for output in sample_output.outputs:\n",
    "\n",
    "                log_prob = torch.zeros((max_seq_len, vocab_size)).fill_(-torch.inf)\n",
    "                sequence = torch.zeros(max_seq_len).fill_(self.tokenizer.eos_token_id).long()\n",
    "    \n",
    "                for i, probs in enumerate(output.logprobs):\n",
    "                    top_tokens = torch.tensor(list(probs.keys()))\n",
    "                    top_values = torch.tensor([lp.logprob for lp in probs.values()])\n",
    "                    log_prob[i, top_tokens] = top_values\n",
    "                    sequence[i] = output.token_ids[i]\n",
    "\n",
    "                logits.append(log_prob)\n",
    "                sequences.append(sequence)\n",
    "\n",
    "        standard_output.logits = logits\n",
    "        standard_output.scores = logits\n",
    "        standard_output.sequences = sequences\n",
    "\n",
    "        return standard_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385977f4-a90f-4065-aabc-7941bcacc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.stat_calculators import StatCalculator\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class InfervLLMCalculator(StatCalculator):\n",
    "    \"\"\"\n",
    "    Performs inference of the model and ensures that output contains\n",
    "    1. logprobas\n",
    "    2. tokens\n",
    "    3. embeddings\n",
    "\n",
    "    For Whitebox model (lm_polygraph.WhiteboxModel), at input texts batch calculates:\n",
    "    * generation texts\n",
    "    * tokens of the generation texts\n",
    "    * probabilities distribution of the generated tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_alternatives: int = 10,\n",
    "        return_embeddings: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_alternatives = n_alternatives\n",
    "        self._return_embeddings = return_embeddings # not supported by vLLM\n",
    "\n",
    "    @staticmethod\n",
    "    def meta_info() -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Returns the statistics and dependencies for the calculator.\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            \"greedy_log_probs\",\n",
    "            \"greedy_logits\",\n",
    "            \"greedy_tokens\",\n",
    "            \"greedy_log_likelihoods\",\n",
    "            \"greedy_tokens_alternatives\",\n",
    "        ], []\n",
    "\n",
    "    def _post_process_logits(self, out, model_inputs, eos_token_id):\n",
    "        cut_logits = []\n",
    "        cut_sequences = []\n",
    "        cut_log_probs = []\n",
    "        cut_alternatives = []\n",
    "        lls = []\n",
    "        \n",
    "        for i in range(len(model_inputs)):\n",
    "        \n",
    "            seq = np.array(out.sequences[i])\n",
    "            length = len(seq)\n",
    "            for j in range(len(seq)):\n",
    "                if seq[j] == eos_token_id:\n",
    "                    length = j + 1\n",
    "                    break\n",
    "                    \n",
    "            tokens = seq[:length].tolist()\n",
    "            cut_sequences.append(tokens)\n",
    "\n",
    "            log_probs = out.scores[i][:length, :]\n",
    "            logits = np.exp(log_probs)\n",
    "            \n",
    "            cut_logits.append(log_probs)\n",
    "            cut_log_probs.append(log_probs)\n",
    "            lls.append([log_probs[j, tokens[j]] for j in range(len(log_probs))])\n",
    "        \n",
    "            cut_alternatives.append([[] for _ in range(length)])\n",
    "            for j in range(length):\n",
    "                lt = logits[j, :]\n",
    "                best_tokens = np.argpartition(lt, -self.n_alternatives)\n",
    "                ln = len(best_tokens)\n",
    "                best_tokens = best_tokens[ln - self.n_alternatives : ln]\n",
    "                for t in best_tokens:\n",
    "                    cut_alternatives[-1][j].append((t, lt[t]))\n",
    "        \n",
    "                cut_alternatives[-1][j].sort(\n",
    "                    key=lambda x: x[0] == cut_sequences[-1][j],\n",
    "                    reverse=True,\n",
    "                )\n",
    "        \n",
    "        result_dict = {\n",
    "            \"greedy_log_probs\": cut_log_probs,\n",
    "            \"greedy_logits\": cut_logits,\n",
    "            \"greedy_tokens\": cut_sequences,\n",
    "            \"greedy_log_likelihoods\": lls,\n",
    "            \"greedy_tokens_alternatives\": cut_alternatives,\n",
    "        }\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        dependencies: Dict[str, np.array],\n",
    "        texts: List[str],\n",
    "        model: Model,\n",
    "        max_new_tokens: int = 100,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculates the statistics of probabilities at each token position in the generation.\n",
    "\n",
    "        Parameters:\n",
    "            dependencies (Dict[str, np.ndarray]): input statistics, can be empty (not used).\n",
    "            texts (List[str]): Input texts batch used for model generation.\n",
    "            model (Model): Model used for generation.\n",
    "            max_new_tokens (int): Maximum number of new tokens at model generation. Default: 100.\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: dictionary with the following items:\n",
    "                - 'greedy_log_probs' (List[List[np.array]]): logarithms of autoregressive\n",
    "                        probability distributions at each token,\n",
    "                - 'greedy_texts' (List[str]): model generations corresponding to the inputs,\n",
    "                - 'greedy_tokens' (List[List[int]]): tokenized model generations,\n",
    "                - 'greedy_log_likelihoods' (List[List[float]]): log-probabilities of the generated tokens.\n",
    "        \"\"\"\n",
    "        out = model.generate(texts, **kwargs)\n",
    "        result_dict = self._post_process_logits(\n",
    "            out, texts, model.tokenizer.eos_token_id\n",
    "        )\n",
    "        if self._return_embeddings:\n",
    "            result_dict.update(\n",
    "                {\"embeddings_decoder\": self._get_embeddings_from_output(out)}\n",
    "            )\n",
    "\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e11d4-6e2d-4d77-8d2f-dd1c3864b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_samples(n_samples, model, batch, **kwargs):\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    logits, sequences = [[] for _ in range(batch_size)], [[] for _ in range(batch_size)]\n",
    "    with torch.no_grad():\n",
    "        for k in range(n_samples):\n",
    "            out = model.generate(batch[\"texts\"], **kwargs)\n",
    "            cur_logits = torch.stack(out.scores, dim=0)\n",
    "            for i in range(batch_size):\n",
    "                sequences[i].append(out.sequences[i])\n",
    "                logits[i].append(cur_logits[i])\n",
    "    sequences = [s for sample_seqs in sequences for s in sample_seqs]\n",
    "    return sequences, sum(logits, [])\n",
    "\n",
    "class SamplingGenerationCalculator(StatCalculator):\n",
    "    \"\"\"\n",
    "    For WhiteboxModelvLLM model, at input texts batch calculates:\n",
    "    * sampled texts\n",
    "    * tokens of the sampled texts\n",
    "    * probabilities of the sampled tokens generation\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def meta_info() -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Returns the statistics and dependencies for the calculator.\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            \"sample_log_probs\",\n",
    "            \"sample_tokens\",\n",
    "            \"sample_texts\",\n",
    "            \"sample_log_likelihoods\",\n",
    "        ], []\n",
    "\n",
    "    def __init__(self, samples_n: int = 10):\n",
    "        super().__init__()\n",
    "        self.samples_n = samples_n\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        dependencies: Dict[str, np.array],\n",
    "        texts: List[str],\n",
    "        model: WhiteboxModelvLLM,\n",
    "        max_new_tokens: int = 100,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculates the statistics of sampling texts.\n",
    "\n",
    "        Parameters:\n",
    "            dependencies (Dict[str, np.ndarray]): input statistics, can be empty (not used).\n",
    "            texts (List[str]): Input texts batch used for model generation.\n",
    "            model (Model): Model used for generation.\n",
    "            max_new_tokens (int): Maximum number of new tokens at model generation. Default: 100.\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: dictionary with the following items:\n",
    "                - 'sample_texts' (List[List[str]]): `samples_n` texts for each input text in the batch,\n",
    "                - 'sample_tokens' (List[List[List[float]]]): tokenized 'sample_texts',\n",
    "                - 'sample_log_probs' (List[List[float]]): sum of the log probabilities at each token of the sampling generation.\n",
    "                - 'sample_log_likelihoods' (List[List[List[float]]]): log probabilities at each token of the sampling generation.\n",
    "        \"\"\"\n",
    "        batch: Dict[str, torch.Tensor] = model.tokenize(texts)\n",
    "        batch[\"texts\"] = texts\n",
    "        sequences, logits = _gen_samples(\n",
    "            self.samples_n,\n",
    "            model,\n",
    "            batch,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        log_probs = [[] for _ in range(len(texts))]\n",
    "        tokens = [[] for _ in range(len(texts))]\n",
    "        texts = [[] for _ in range(len(texts))]\n",
    "        log_likelihoods = [[] for _ in range(len(texts))]\n",
    "        if model.model_type == \"Seq2SeqLM\":\n",
    "            sequences = [seq[1:] for seq in sequences]\n",
    "        for i in range(len(logits)):\n",
    "            log_prob, ll, toks = 0, [], []\n",
    "            inp_size = (\n",
    "                len(batch[\"input_ids\"][int(i / self.samples_n)])\n",
    "                if model.model_type == \"CausalLM\"\n",
    "                else 0\n",
    "            )\n",
    "            for j in range(len(sequences[i]) - inp_size):\n",
    "                cur_token = sequences[i][j + inp_size].item()\n",
    "                log_prob += logits[i][j][cur_token].item()\n",
    "                if cur_token == model.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                ll.append(logits[i][j][cur_token].item())\n",
    "                toks.append(cur_token)\n",
    "\n",
    "            log_likelihoods[int(i / self.samples_n)].append(ll)\n",
    "            log_probs[int(i / self.samples_n)].append(log_prob)\n",
    "            tokens[int(i / self.samples_n)].append(toks)\n",
    "            texts[int(i / self.samples_n)].append(model.tokenizer.decode(toks))\n",
    "\n",
    "        return {\n",
    "            \"sample_log_likelihoods\": log_likelihoods,\n",
    "            \"sample_log_probs\": log_probs,\n",
    "            \"sample_tokens\": tokens,\n",
    "            \"sample_texts\": texts,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27dc09-d5f9-408a-a673-b99ff28ba3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.stat_calculators.greedy_alternatives_nli import GreedyAlternativesNLICalculator\n",
    "from lm_polygraph.stat_calculators.cross_encoder_similarity import CrossEncoderSimilarityMatrixCalculator\n",
    "from lm_polygraph.stat_calculators.semantic_matrix import SemanticMatrixCalculator\n",
    "from lm_polygraph.stat_calculators.semantic_classes import SemanticClassesCalculator\n",
    "\n",
    "from lm_polygraph.estimators import MaximumSequenceProbability, ClaimConditionedProbability, DegMat, SemanticEntropy, SAR\n",
    "\n",
    "from lm_polygraph.utils.deberta import Deberta\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_adapter = WhiteboxModelvLLM(llm, device)\n",
    "\n",
    "calc_infer_llm = InfervLLMCalculator()\n",
    "nli_model = Deberta(device=device)\n",
    "nli_model.setup()\n",
    "calc_nli = GreedyAlternativesNLICalculator(nli_model=nli_model)\n",
    "\n",
    "calc_samples = SamplingGenerationCalculator()\n",
    "calc_cross_encoder = CrossEncoderSimilarityMatrixCalculator()\n",
    "calc_semantic_matrix = SemanticMatrixCalculator(nli_model=nli_model)\n",
    "calc_semantic_classes = SemanticClassesCalculator()\n",
    "\n",
    "estimators = [MaximumSequenceProbability(), \n",
    "              ClaimConditionedProbability(),\n",
    "              DegMat(), \n",
    "              SemanticEntropy(), \n",
    "              SAR()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be0136-5d1f-4103-9d10-54a60f127402",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(chat_messages, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "for batch in data_loader:\n",
    "    deps = {\"input_texts\": batch}\n",
    "    deps.update(calc_infer_llm(\n",
    "        deps, texts=batch, model=model_adapter, sampling_params=sampling_params))\n",
    "    deps.update(calc_nli(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_samples(deps, texts=batch, model=model_adapter, sampling_params=sampling_params))\n",
    "    deps.update(calc_cross_encoder(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_semantic_matrix(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_semantic_classes(deps, texts=batch, model=model_adapter))\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(deps['greedy_tokens'])\n",
    "    ues = []\n",
    "    for estimator in estimators:\n",
    "        uncertainty_scores = estimator(deps)\n",
    "        ues.append((str(estimator), uncertainty_scores))\n",
    "\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(\"Output:\", text)\n",
    "        for scores in ues:\n",
    "            print(f\"Uncertainty score by {scores[0]}: {scores[1][i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d4b06-aab7-41ed-ace2-a1022c21307f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lmp_proposal]",
   "language": "python",
   "name": "conda-env-lmp_proposal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
