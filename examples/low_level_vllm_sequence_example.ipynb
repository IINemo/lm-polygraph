{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025e26e-fd7f-44b6-88d7-5876439a5ab0",
   "metadata": {},
   "source": [
    "# Specify HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111f938-bc8c-4b82-82a1-fce490bc8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda:0\"\n",
    "dataset_name = \"../workdir/data/triviaqa.csv\"\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a3862-77d1-4bb4-8423-1f86f3a58b54",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=model_name_or_path, gpu_memory_utilization=0.5)\n",
    "sampling_params = SamplingParams(max_tokens=30, logprobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many fingers on a coala's foot?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Yesterday?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Кто спел песню Кукла Колдуна?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False) for m in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04e238",
   "metadata": {},
   "source": [
    "# Infer LLM and get uncertainty scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4ceab-7e7f-4087-a90a-ba07335df068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from lm_polygraph.utils.model import Model\n",
    "from vllm import LLM\n",
    "\n",
    "class WhiteboxModelvLLM(Model):\n",
    "    \"\"\"Basic whitebox model adapter for using vLLM in stat calculators and uncertainty estimators.\"\"\"\n",
    "\n",
    "    def __init__(self, model: LLM):\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.get_tokenizer()\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.model.generate(*args, **kwargs)\n",
    "\n",
    "    def tokenize(self, *args, **kwargs):\n",
    "        return self.tokenizer(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.generate(*args, **kwargs)\n",
    "\n",
    "    def generate_texts(self, input_texts: List[str], **args):\n",
    "        outputs = self.generate(input_texts, args.pop(\"sampling_params\"))\n",
    "        texts = [output.outputs[0].text for output in outputs]\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1b1f4-2ea6-4337-b681-038529bfb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.stat_calculators import StatCalculator\n",
    "from lm_polygraph.utils.model import Model\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class InfervLLMCalculator(StatCalculator):\n",
    "    \"\"\"\n",
    "    Performs inference of the model and ensures that output contains\n",
    "    1. logprobas\n",
    "    2. tokens\n",
    "    3. embeddings\n",
    "\n",
    "    For Whitebox model (lm_polygraph.WhiteboxModel), at input texts batch calculates:\n",
    "    * generation texts\n",
    "    * tokens of the generation texts\n",
    "    * probabilities distribution of the generated tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_alternatives: int = 10,\n",
    "        return_embeddings: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_alternatives = n_alternatives\n",
    "        self._return_embeddings = return_embeddings # not supported by vLLM\n",
    "\n",
    "    @staticmethod\n",
    "    def meta_info() -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Returns the statistics and dependencies for the calculator.\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            \"greedy_log_probs\",\n",
    "            \"greedy_logits\",\n",
    "            \"greedy_tokens\",\n",
    "            \"greedy_log_likelihoods\",\n",
    "            \"greedy_tokens_alternatives\",\n",
    "        ], []\n",
    "\n",
    "    def _post_process_logits(self, out, model_inputs, vocab_size, eos_token_id):\n",
    "        cut_logits = []\n",
    "        cut_sequences = []\n",
    "        cut_log_probs = []\n",
    "        cut_alternatives = []\n",
    "        lls = []\n",
    "        \n",
    "        for i in range(len(model_inputs)):\n",
    "        \n",
    "            seq = np.array(out[i].outputs[0].token_ids)\n",
    "            length = len(seq)\n",
    "            for j in range(len(seq)):\n",
    "                if seq[j] == eos_token_id:\n",
    "                    length = j + 1\n",
    "                    break\n",
    "                    \n",
    "            tokens = seq[:length].tolist()\n",
    "            cut_sequences.append(tokens)\n",
    "        \n",
    "            top_log_probs = out[i].outputs[0].logprobs\n",
    "            log_probs = np.zeros((len(top_log_probs), vocab_size))\n",
    "            for i, probs in enumerate(top_log_probs):\n",
    "                top_tokens = np.array(list(probs.keys()))\n",
    "                top_values = np.array([lp.logprob for lp in probs.values()])\n",
    "                log_probs[i, :] = -np.inf\n",
    "                log_probs[i, top_tokens] = top_values\n",
    "        \n",
    "            log_probs = log_probs[:length, :]\n",
    "            logits = np.exp(log_probs)\n",
    "            \n",
    "            cut_logits.append(log_probs)\n",
    "            cut_log_probs.append(log_probs)\n",
    "            lls.append([log_probs[j, tokens[j]] for j in range(len(log_probs))])\n",
    "        \n",
    "            cut_alternatives.append([[] for _ in range(length)])\n",
    "            for j in range(length):\n",
    "                lt = logits[j, :]\n",
    "                best_tokens = np.argpartition(lt, -self.n_alternatives)\n",
    "                ln = len(best_tokens)\n",
    "                best_tokens = best_tokens[ln - self.n_alternatives : ln]\n",
    "                for t in best_tokens:\n",
    "                    cut_alternatives[-1][j].append((t, lt[t]))\n",
    "        \n",
    "                cut_alternatives[-1][j].sort(\n",
    "                    key=lambda x: x[0] == cut_sequences[-1][j],\n",
    "                    reverse=True,\n",
    "                )\n",
    "        \n",
    "        result_dict = {\n",
    "            \"greedy_log_probs\": cut_log_probs,\n",
    "            \"greedy_logits\": cut_logits,\n",
    "            \"greedy_tokens\": cut_sequences,\n",
    "            \"greedy_log_likelihoods\": lls,\n",
    "            \"greedy_tokens_alternatives\": cut_alternatives,\n",
    "        }\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        dependencies: Dict[str, np.array],\n",
    "        texts: List[str],\n",
    "        model: Model,\n",
    "        max_new_tokens: int = 100,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculates the statistics of probabilities at each token position in the generation.\n",
    "\n",
    "        Parameters:\n",
    "            dependencies (Dict[str, np.ndarray]): input statistics, can be empty (not used).\n",
    "            texts (List[str]): Input texts batch used for model generation.\n",
    "            model (Model): Model used for generation.\n",
    "            max_new_tokens (int): Maximum number of new tokens at model generation. Default: 100.\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: dictionary with the following items:\n",
    "                - 'greedy_log_probs' (List[List[np.array]]): logarithms of autoregressive\n",
    "                        probability distributions at each token,\n",
    "                - 'greedy_texts' (List[str]): model generations corresponding to the inputs,\n",
    "                - 'greedy_tokens' (List[List[int]]): tokenized model generations,\n",
    "                - 'greedy_log_likelihoods' (List[List[float]]): log-probabilities of the generated tokens.\n",
    "        \"\"\"\n",
    "        out = model.generate(texts, **kwargs)\n",
    "        vocab_size = max(model.tokenizer.vocab_size, max(model.tokenizer.added_tokens_decoder.keys()))\n",
    "        result_dict = self._post_process_logits(\n",
    "            out, texts, vocab_size, model.tokenizer.eos_token_id\n",
    "        )\n",
    "        if self._return_embeddings:\n",
    "            result_dict.update(\n",
    "                {\"embeddings_decoder\": self._get_embeddings_from_output(out)}\n",
    "            )\n",
    "\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27dc09-d5f9-408a-a673-b99ff28ba3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.stat_calculators.greedy_alternatives_nli import GreedyAlternativesNLICalculator\n",
    "from lm_polygraph.estimators.claim_conditioned_probability import ClaimConditionedProbability\n",
    "from lm_polygraph.utils.deberta import Deberta\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_adapter = WhiteboxModelvLLM(llm)\n",
    "\n",
    "calc_infer_llm = InfervLLMCalculator()\n",
    "nli_model = Deberta(device=device)\n",
    "nli_model.setup()\n",
    "calc_nli = GreedyAlternativesNLICalculator(nli_model=nli_model)\n",
    "\n",
    "estimator = ClaimConditionedProbability()\n",
    "\n",
    "data_loader = DataLoader(chat_messages, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "for batch in data_loader:\n",
    "    deps = {}\n",
    "    deps.update(calc_infer_llm(\n",
    "        deps, texts=batch, model=model_adapter, sampling_params=sampling_params))\n",
    "    deps.update(calc_nli(deps, texts=None, model=model_adapter))\n",
    "\n",
    "    uncertainty_scores = estimator(deps)\n",
    "    generated_texts = tokenizer.batch_decode(deps['greedy_tokens'])\n",
    "    \n",
    "    for text, ue_score in zip(generated_texts, uncertainty_scores):\n",
    "        print(\"Output:\", text)\n",
    "        print(\"Uncertainty score:\", ue_score)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lmp_proposal]",
   "language": "python",
   "name": "conda-env-lmp_proposal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
