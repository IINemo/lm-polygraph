{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6958a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.utils.model import WhiteboxModel, BlackboxModel\n",
    "from lm_polygraph.utils.manager import estimate_uncertainty\n",
    "from lm_polygraph.estimators import MaximumTokenProbability, LexicalSimilarity, SemanticEntropy, PointwiseMutualInformation, NumSemSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7a7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhiteboxModel.from_pretrained(\n",
    "    'bigscience/bloomz-560m',\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "476d7df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lm_polygraph.utils.model.WhiteboxModel"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247f5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x135fa20d0>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[-0.24327608942985535, -0.924824595451355, -0.9097719788551331, -0.995514988899231, -0.9928673505783081])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MaximumTokenProbability()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8292b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x16defe250>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[-1.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LexicalSimilarity('rougeL')\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a906db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x1708ab150>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[3.257049634584728])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SemanticEntropy()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f3c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate args: {'input_ids': tensor([[ 64393,  14591,    267,   3509,   2782,   1620,    267,  10512,  27566,\n",
      "           5268, 127140,    427]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x111b7dad0>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' play with her dolls. She was a little girl who was very clever and had a great imagination.'], generation_tokens=[[7229, 1002, 3809, 17486, 86, 17, 15114, 1620, 267, 10512, 27566, 5268, 1620, 5636, 149014, 530, 3866, 267, 10087, 113763, 17, 2]], uncertainty=[2.019789218902588, -5.537353157997131, -4.312102556228638, -5.030910968780518, -2.7699881196022034, -10.396235346794128, -9.28441572189331, -11.279396176338196, -8.391041040420532, -7.909592628479004, -4.460748910903931, -5.949978590011597, -7.363903284072876, -8.148085594177246, -2.7360548973083496, -8.350634574890137, -5.361073970794678, -5.55866664648056, -6.428493142127991, -7.301618158817291, -10.471122086048126])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = PointwiseMutualInformation()\n",
    "estimate_uncertainty(model, estimator, input_text='Once upon a time there was a little girl who liked to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93cda59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[10560,   632,   368,  6084, 34495,  8874,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x1709c1e50>]}\n",
      "generated answers: ['What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film?>:: The Last Ship</s>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' The Last Ship'], generation_tokens=[[1387, 46326, 158384, 2]], uncertainty=[1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = NumSemSets(verbose=True)\n",
    "estimate_uncertainty(model, estimator, input_text='What is the most interesting film?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc03fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x16e1af450>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[-1.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WhiteboxModel.from_pretrained(\n",
    "    'bigscience/bloomz-560m',\n",
    "    device='cpu',\n",
    ")\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff1856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x170966410>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Failed, no normalization..."
     ]
    }
   ],
   "source": [
    "res = estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb84386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b63635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=['a republic'], generation_tokens=None, uncertainty=[-0.0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lm_polygraph.utils.model import WhiteboxModel, BlackboxModel\n",
    "from lm_polygraph.utils.manager import estimate_uncertainty\n",
    "from lm_polygraph.estimators import MaximumTokenProbability, LexicalSimilarity, SemanticEntropy, PointwiseMutualInformation, NumSemSets\n",
    "\n",
    "API_TOKEN = YOUR_API_TOKEN\n",
    "MODEL_ID = 'google/t5-small-ssm-nq'\n",
    "\n",
    "model = BlackboxModel.from_huggingface(hf_api_token=API_TOKEN, hf_model_id=MODEL_ID, openai_api_key = None, openai_model_path = None)\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d3024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
