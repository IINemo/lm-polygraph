{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6958a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.utils.model import WhiteboxModel, BlackboxModel\n",
    "from lm_polygraph.utils.manager import estimate_uncertainty\n",
    "from lm_polygraph.estimators import MaximumTokenProbability, LexicalSimilarity, SemanticEntropy, PointwiseMutualInformation, NumSemSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7a7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhiteboxModel.from_pretrained(\n",
    "    'bigscience/bloomz-560m',\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "476d7df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lm_polygraph.utils.model.WhiteboxModel"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247f5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x135fa20d0>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[-0.24327608942985535, -0.924824595451355, -0.9097719788551331, -0.995514988899231, -0.9928673505783081])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MaximumTokenProbability()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8292b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x16defe250>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[-1.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LexicalSimilarity('rougeL')\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a906db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'num_return_sequences': 1}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x1708ab150>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[3.257049634584728])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SemanticEntropy()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f3c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate args: {'input_ids': tensor([[ 64393,  14591,    267,   3509,   2782,   1620,    267,  10512,  27566,\n",
      "           5268, 127140,    427]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x111b7dad0>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' play with her dolls. She was a little girl who was very clever and had a great imagination.'], generation_tokens=[[7229, 1002, 3809, 17486, 86, 17, 15114, 1620, 267, 10512, 27566, 5268, 1620, 5636, 149014, 530, 3866, 267, 10087, 113763, 17, 2]], uncertainty=[2.019789218902588, -5.537353157997131, -4.312102556228638, -5.030910968780518, -2.7699881196022034, -10.396235346794128, -9.28441572189331, -11.279396176338196, -8.391041040420532, -7.909592628479004, -4.460748910903931, -5.949978590011597, -7.363903284072876, -8.148085594177246, -2.7360548973083496, -8.350634574890137, -5.361073970794678, -5.55866664648056, -6.428493142127991, -7.301618158817291, -10.471122086048126])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = PointwiseMutualInformation()\n",
    "estimate_uncertainty(model, estimator, input_text='Once upon a time there was a little girl who liked to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93cda59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[10560,   632,   368,  6084, 34495,  8874,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x1709c1e50>]}\n",
      "generated answers: ['What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film?>:: The Last Ship</s>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>', 'What is the most interesting film? The Last Ship</s><pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' The Last Ship'], generation_tokens=[[1387, 46326, 158384, 2]], uncertainty=[1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = NumSemSets(verbose=True)\n",
    "estimate_uncertainty(model, estimator, input_text='What is the most interesting film?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc03fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x16e1af450>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Failed, no normalization..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(generation_text=[' President of the United States'], generation_tokens=[[14032, 461, 368, 8097, 10650, 2]], uncertainty=[-1.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WhiteboxModel.from_pretrained(\n",
    "    'bigscience/bloomz-560m',\n",
    "    device='cpu',\n",
    ")\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff1856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteboxModel.generate_texts args: {'max_length': 256, 'min_length': 2, 'do_sample': True, 'num_beams': 1, 'temperature': 1.0, 'top_p': 1.0, 'repetition_penalty': 1, 'top_k': 1, 'num_return_sequences': 10, 'return_dict_in_generate': True}\n",
      "WhiteboxModel.generate args: {'input_ids': tensor([[57647,   632, 20773, 46275,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'output_scores': True, 'return_dict_in_generate': True, 'max_new_tokens': 256, 'min_length': 2, 'output_attentions': True, 'output_hidden_states': True, 'temperature': 1.0, 'top_k': 1, 'top_p': 1.0, 'do_sample': False, 'num_beams': 1, 'repetition_penalty': 1, 'suppress_tokens': [], 'num_return_sequences': 1, 'logits_processor': [<lm_polygraph.stat_calculators.greedy_probs.ScoresProcessor object at 0x170966410>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Failed, no normalization..."
     ]
    }
   ],
   "source": [
    "res = estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb84386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b63635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.stat_calculators [<lm_polygraph.stat_calculators.sample.BlackboxSamplingGenerationCalculator object at 0x166634210>, <lm_polygraph.stat_calculators.greedy_probs.BlackboxGreedyTextsCalculator object at 0x16660a8d0>]\n",
      "BlackBox.generate_texts args: {'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'presence_penalty': 0, 'n': 10}\n",
      "hf\n",
      "output [{'generated_text': 'a republic'}]\n",
      "texts ['a republic']\n",
      "BlackBox.generate_texts args: {'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'presence_penalty': 0, 'n': 1}\n",
      "hf\n",
      "output [{'generated_text': 'a republic'}]\n",
      "texts ['a republic']\n",
      "> \u001b[0;32m/Users/sergeypetrakov/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/lm_polygraph/utils/manager.py\u001b[0m(77)\u001b[0;36mestimate_uncertainty\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     75 \u001b[0;31m    \u001b[0mue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 77 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mcan_normalize_ue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sequence'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     79 \u001b[0;31m            \u001b[0mue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_ue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "model_path None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m ue_method \u001b[39m=\u001b[39m LexicalSimilarity()\n\u001b[1;32m     10\u001b[0m input_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWho is George Bush?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m estimate_uncertainty(model, ue_method, input_text\u001b[39m=\u001b[39;49minput_text)\n",
      "File \u001b[0;32m~/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/lm_polygraph/utils/manager.py:77\u001b[0m, in \u001b[0;36mestimate_uncertainty\u001b[0;34m(model, estimator, input_text, target_text)\u001b[0m\n\u001b[1;32m     75\u001b[0m ue \u001b[39m=\u001b[39m man\u001b[39m.\u001b[39mestimations[estimator\u001b[39m.\u001b[39mlevel, \u001b[39mstr\u001b[39m(estimator)]\n\u001b[1;32m     76\u001b[0m pdb\u001b[39m.\u001b[39mset_trace()\n\u001b[0;32m---> 77\u001b[0m \u001b[39mif\u001b[39;00m can_normalize_ue(estimator, model\u001b[39m.\u001b[39;49mmodel_path):\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m estimator\u001b[39m.\u001b[39mlevel \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     79\u001b[0m         ue \u001b[39m=\u001b[39m normalize_ue(estimator, model\u001b[39m.\u001b[39mmodel_path, ue[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/lm_polygraph/utils/normalize.py:37\u001b[0m, in \u001b[0;36mcan_normalize_ue\u001b[0;34m(est, model_path, cache_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcan_normalize_ue\u001b[39m(est: Estimator, model_path: \u001b[39mstr\u001b[39m, cache_path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m DEFAULT_CACHE_PATH) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m normalization_bounds_present(est, model_path, \u001b[39m'\u001b[39;49m\u001b[39mnormalization\u001b[39;49m\u001b[39m'\u001b[39;49m, cache_path)\n",
      "File \u001b[0;32m~/Documents/Documents/hf_connect/hf_connect/lib/python3.11/site-packages/lm_polygraph/utils/normalize.py:18\u001b[0m, in \u001b[0;36mnormalization_bounds_present\u001b[0;34m(est, model_path, directory, cache_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalization_bounds_present\u001b[39m(est: Estimator, model_path: \u001b[39mstr\u001b[39m, directory: \u001b[39mstr\u001b[39m, cache_path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m DEFAULT_CACHE_PATH) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel_path\u001b[39m\u001b[39m\"\u001b[39m, model_path)\n\u001b[0;32m---> 18\u001b[0m     archive_path \u001b[39m=\u001b[39m model_path\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     19\u001b[0m     filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cache_path, archive_path)\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(filepath):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.utils.model import WhiteboxModel, BlackboxModel\n",
    "from lm_polygraph.utils.manager import estimate_uncertainty\n",
    "from lm_polygraph.estimators import MaximumTokenProbability, LexicalSimilarity, SemanticEntropy, PointwiseMutualInformation, NumSemSets\n",
    "\n",
    "API_TOKEN = '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'\n",
    "MODEL_ID = 'google/t5-small-ssm-nq'\n",
    "\n",
    "model = BlackboxModel.from_huggingface(hf_api_token=API_TOKEN, hf_model_id=MODEL_ID, openai_api_key = None, openai_model_path = None)\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d3024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
