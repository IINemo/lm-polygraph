{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original inference with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-25 20:32:52,341] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemshelmanov/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/artemshelmanov/conda/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eddc17b7d9489f8b171e6ec5dd535b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output:\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "user\n",
      "\n",
      "Write a short story about a robot learning to paint.assistant\n",
      "\n",
      "**The Birth of Art**\n",
      "\n",
      "In a small, dimly lit workshop, a brilliant inventor, Dr. Emma Taylor, had been working on a top-secret project for months. Her latest creation, a sleek and advanced robot named Nova, stood before her, its metallic body gleaming under the soft glow of the workshop lights. Nova was designed to learn and adapt at an exponential rate, and Dr. Taylor had a specific goal in mind for her creation.\n",
      "\n",
      "\"Today, Nova,\" she said, her voice filled with excitement, \"we're going to learn something new. Something beautiful.\"\n",
      "\n",
      "Dr. Taylor led Nova to a large, blank canvas, set up a easel, and handed the robot a paintbrush. Nova's advanced sensors and algorithms quickly analyzed the brush, the canvas, and the colors available. It was a blank slate, ready to absorb knowledge.\n",
      "\n",
      "\"Begin by painting a simple line,\" Dr. Taylor instructed. \"Red.\"\n",
      "\n",
      "Nova's mechanical arm moved, and a vibrant red line\n"
     ]
    }
   ],
   "source": [
    "# Original LLM inference without uncertainty estimation\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "llm = llm.to(device)\n",
    "\n",
    "# Example prompt \n",
    "prompt = \"Write a short story about a robot learning to paint.\\n\"\n",
    "\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm.generate(\n",
    "    **inputs,  # Unpack the tokenized inputs\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "print(\"LLM output:\")\n",
    "print(tokenizer.decode(output.sequences[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155de5d7a7a14ddc860b3145164dbda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output:\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "user\n",
      "\n",
      "Write a short story about a robot learning to paint.assistant\n",
      "\n",
      "In a small, dimly lit studio, a robot named Zeta sat in front of a canvas, its metal body swaying slightly as it adjusted its position. The studio was quiet, except for the soft hum of Zeta's systems and the faint sound of a clock ticking in the background.\n",
      "\n",
      "Zeta's creator, a brilliant scientist named Dr. Rachel, stood beside the robot, watching with a mixture of excitement and trepidation. She had designed Zeta to learn and adapt, and painting was the next step in its development.\n",
      "\n",
      "\"Okay, Zeta,\" Dr. Rachel said, her voice calm and soothing. \"Today, we're going to try painting. I want you to create a beautiful landscape, with rolling hills and a bright blue sky.\"\n",
      "\n",
      "Zeta's advanced eyes scanned the canvas, taking in the blank white space. It whirred softly as it processed the task, its neural networks analyzing the concept of painting and the desired outcome.\n",
      "\n",
      "The robot's mechanical arm,\n",
      "Uncertainty score:  [0.3687719]\n"
     ]
    }
   ],
   "source": [
    "# LLM inference with uncertainty estimation\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ============== Addition Imports ===============\n",
    "from lm_polygraph.estimators import MeanTokenEntropy\n",
    "from lm_polygraph.stat_calculators import InferCausalLMCalculator, EntropyCalculator\n",
    "from lm_polygraph.utils.causal_lm_with_uncertainty import CausalLMWithUncertainty\n",
    "# ===============================================\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Loading standard LLM\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "llm = llm.to(device)\n",
    "\n",
    "# ======= Wrapping LLM with uncertainty estimator =========\n",
    "stat_calculators = [InferCausalLMCalculator(tokenize=False),\n",
    "                    EntropyCalculator()]\n",
    "estimator = MeanTokenEntropy()\n",
    "llm_with_uncertainty = CausalLMWithUncertainty(llm, tokenizer, stat_calculators, estimator)\n",
    "# =========================================================\n",
    "\n",
    "# Example prompt \n",
    "prompts = [\"Write a short story about a robot learning to paint.\\n\"]\n",
    "\n",
    "chats = [[{\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "chat_prompts = tokenizer.apply_chat_template(chats, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(chat_prompts, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm_with_uncertainty.generate(\n",
    "    **inputs,  \n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"LLM output:\")\n",
    "print(tokenizer.decode(output.sequences[0], skip_special_tokens=True))\n",
    "\n",
    "# ================ Printing uncertainty score ================\n",
    "print(\"Uncertainty score: \", output.uncertainty_score)\n",
    "# ============================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
