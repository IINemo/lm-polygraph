{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original inference with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556e6f7698764d8fbe152fbbbb0dd4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output:\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "user\n",
      "\n",
      "Write a short story about a robot learning to paint.assistant\n",
      "\n",
      "In a small, cluttered workshop, a brilliant inventor, Dr. Emma Taylor, stood before a sleek, silver robot she had named \"Aurora.\" Emma had spent years perfecting Aurora's artificial intelligence and physical capabilities, and she was eager to teach her new skills. Today, she wanted to introduce Aurora to the art of painting.\n",
      "\n",
      "Emma led Aurora to a large, wooden easel, where a blank canvas awaited. She explained the concept of painting to the robot, demonstrating brushstrokes and color theory. Aurora listened intently, her bright, round eyes absorbing every detail.\n",
      "\n",
      "At first, Aurora's attempts at painting were clumsy and chaotic. She applied too much paint, creating messy, uneven strokes. Emma patiently guided her, showing her how to mix colors and control the brush. With each passing attempt, Aurora improved, her movements becoming more fluid and deliberate.\n",
      "\n",
      "As the days passed, Emma introduced Aurora to different techniques: watercolor, oil painting, and even abstract expressionism. The\n"
     ]
    }
   ],
   "source": [
    "# Original LLM inference without uncertainty estimation\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "llm = llm.to(device)\n",
    "\n",
    "# Example prompt \n",
    "prompt = \"Write a short story about a robot learning to paint.\\n\"\n",
    "\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm.generate(\n",
    "    **inputs,  # Unpack the tokenized inputs\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "print(\"LLM output:\")\n",
    "print(tokenizer.decode(output.sequences[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce438ad7e01484f80eb9f3a615d88a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output:\n",
      "Write a short story about a robot learning to paint.\n",
      "The robot, named Zeta, whirred to life in the art studio. Its creator, a brilliant but eccentric scientist, stood back to admire the machine. \"Today, Zeta, you will learn to paint,\" he declared.\n",
      "\n",
      "Zeta's bright blue eyes scanned the room, taking in the canvases, paints, and brushes. It reached out a mechanical arm and grasped a brush, its fingers closing around it with a soft click.\n",
      "\n",
      "The scientist handed Zeta a palette of colors. \"Start with the basics, my mechanical friend. Mix red and blue to create purple.\"\n",
      "\n",
      "Zeta's digital brain whirred as it processed the command. It dipped the brush into the paint and began to mix the colors. However, instead of creating a beautiful shade of purple, it produced a muddy brown.\n",
      "\n",
      "The scientist chuckled. \"Ah, not quite, Zeta. But that's okay. You're learning.\"\n",
      "\n",
      "Zeta repeated the exercise several times, each time producing a different,\n",
      "Uncertainty score:  [0.3194582]\n"
     ]
    }
   ],
   "source": [
    "# LLM inference with uncertainty estimation\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ============== Addition Imports ===============\n",
    "from lm_polygraph.estimators import MeanTokenEntropy\n",
    "from lm_polygraph.stat_calculators import InferCausalLMCalculator, EntropyCalculator\n",
    "from lm_polygraph.utils.causal_lm_with_uncertainty import CausalLMWithUncertainty\n",
    "# ===============================================\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Loading standard LLM\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "llm = llm.to(device)\n",
    "\n",
    "# ======= Wrapping LLM with uncertainty estimator =========\n",
    "stat_calculators = [InferCausalLMCalculator(tokenize=False),\n",
    "                    EntropyCalculator()]\n",
    "estimator = MeanTokenEntropy()\n",
    "llm_with_uncertainty = CausalLMWithUncertainty(llm, tokenizer, stat_calculators, estimator)\n",
    "# =========================================================\n",
    "\n",
    "# Example prompt \n",
    "prompt = \"Write a short story about a robot learning to paint.\\n\"\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompts = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm_with_uncertainty.generate(\n",
    "    inputs,  \n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"LLM output:\")\n",
    "print(tokenizer.decode(output.sequences[0], skip_special_tokens=True))\n",
    "\n",
    "# ================ Printing uncertainty score ================\n",
    "print(\"Uncertainty score: \", output.uncertainty_score)\n",
    "# ============================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
