{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7520ef",
   "metadata": {},
   "source": [
    "# Low-Level Examples with vLLM\n",
    "Here we present low-level examples of integrating LM-Polygraph with vLLM for faster inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26144c59",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7111f938-bc8c-4b82-82a1-fce490bc8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda:0\"\n",
    "dataset_name = \"../workdir/data/triviaqa.csv\"\n",
    "batch_size = 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=model_name_or_path, gpu_memory_utilization=0.70)\n",
    "sampling_params = SamplingParams(max_tokens=30, logprobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ded6c",
   "metadata": {},
   "source": [
    "## Texts to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b10d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How many fingers on a coala's foot?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who sang a song Yesterday?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Кто спел песню Кукла Колдуна?\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate into French: 'I want a small cup of coffee'\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "chat_messages = [tokenizer.apply_chat_template(m, tokenize=False) for m in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04e238",
   "metadata": {},
   "source": [
    "## Infer LLM and get uncertainty scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27dc09-d5f9-408a-a673-b99ff28ba3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.model_adapters import WhiteboxModelvLLM\n",
    "from lm_polygraph.stat_calculators.greedy_alternatives_nli import GreedyAlternativesNLICalculator\n",
    "from lm_polygraph.stat_calculators.cross_encoder_similarity import CrossEncoderSimilarityMatrixCalculator\n",
    "from lm_polygraph.stat_calculators.semantic_matrix import SemanticMatrixCalculator\n",
    "from lm_polygraph.stat_calculators.semantic_classes import SemanticClassesCalculator\n",
    "from lm_polygraph.stat_calculators.greedy_probs import GreedyProbsCalculator\n",
    "from lm_polygraph.stat_calculators.sample import SamplingGenerationCalculator\n",
    "from lm_polygraph.estimators import MaximumSequenceProbability, ClaimConditionedProbability, DegMat, SemanticEntropy, SAR\n",
    "from lm_polygraph.utils.deberta import Deberta\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_adapter = WhiteboxModelvLLM(llm, sampling_params=sampling_params, device=device)\n",
    "\n",
    "calc_infer_llm = GreedyProbsCalculator()\n",
    "nli_model = Deberta(device=device)\n",
    "nli_model.setup()\n",
    "calc_nli = GreedyAlternativesNLICalculator(nli_model=nli_model)\n",
    "\n",
    "calc_samples = SamplingGenerationCalculator()\n",
    "calc_cross_encoder = CrossEncoderSimilarityMatrixCalculator()\n",
    "calc_semantic_matrix = SemanticMatrixCalculator(nli_model=nli_model)\n",
    "calc_semantic_classes = SemanticClassesCalculator()\n",
    "\n",
    "# You can use one of the estimators from the library, here, just for example, we use multiple estimators\n",
    "estimators = [MaximumSequenceProbability(), \n",
    "              ClaimConditionedProbability(),\n",
    "              DegMat(), \n",
    "              SemanticEntropy(), \n",
    "              SAR()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be0136-5d1f-4103-9d10-54a60f127402",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(chat_messages, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "for batch in data_loader:\n",
    "    deps = {\"input_texts\": batch}\n",
    "    deps.update(calc_infer_llm(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_nli(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_samples(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_cross_encoder(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_semantic_matrix(deps, texts=batch, model=model_adapter))\n",
    "    deps.update(calc_semantic_classes(deps, texts=batch, model=model_adapter))\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(deps['greedy_tokens'])\n",
    "    ues = []\n",
    "    for estimator in estimators:\n",
    "        uncertainty_scores = estimator(deps)\n",
    "        ues.append((str(estimator), uncertainty_scores))\n",
    "\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(\"Output:\", text)\n",
    "        for scores in ues:\n",
    "            print(f\"Uncertainty score by {scores[0]}: {scores[1][i]}\")\n",
    "        \n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
